{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Noblit Vaporware warning: much of the content below is hypothetical. Noblit is an embeddable append-only database. The database records a history of immutable (entity, attribute, value) tuples. Tuples can be asserted and retracted. A retraction is recorded as a new fact; it is not a delete. Any historical state of the database can be reproduced, and the history is first-class and queryable. Features Embeddable. Noblit comes as a library that you link into your application. Global installation or daemons are not necessary. Simple but flexible data model. Noblit stores (entity, attribute, value) tuples. A traditional relational model can be expressed like this, but less rigid structures such as graphs can be expressed too. Machine-friendly relational queries. Queries are data structures rather than strings. There is no need for string formatting or escaping, and the full power of the host language is available to safely generate and compose queries. Point in time queries. Any historical state of the database can be reproduced and queried efficiently. Reified schema . Noblit stores the schema as entities in the database itself. The schema can be evolved through normal assertions and retractions, and the full migration history is available. Reified transactions . Transactions in Noblit are entities that can have attributes like any other entity. Possibilities include the transaction timestamp and the user who initiated the transaction. Transactions can be inspected in queries. Profile-guided query optimization . While queries are declarative, statement order affects the query plan. An explicit query optimizer can optimize a given query by measuring how alternatives perform, instead of having to estimate from statistics. Comparison Noblit is heavily inspired by Datomic , especially by its data model. In comparison to other databases such as SQLite and Postgres , Noblit positions itself as follows: Client-server Embedded Mutable Postgres SQLite Immutable Datomic Noblit Noblit combines the low operational overhead of an embedded database with the simplicity of an append-only database with value semantics. In comparison to Datomic, Noblit puts more focus on static type safety. This makes schemas more rigid. On the one hand this impedes iterating quickly, but on the other hand it helps to provide data consistency in the long term. Datomic is a distributed system running on the JVM , while Noblit is a small native library. Another attempt at an embedded database inspired by Datomic was Mentat . Both Noblit and Mentat happen to be written in Rust. Mentat is a layer on top of SQLite, whereas Noblit has its own storage backend. Mentat exists but is unmaintained, Noblit is vaporware. Goals An embeddable library for queries and transactions. Relational queries in a machine-first query format. Durable storage on disk, with atomic transactions. Easy to build, few or zero dependencies. Handling moderate write workloads and moderately sized data. Storing datasets that do not fit in working memory. The result of a query and data to insert must fit though. Enabling \u2014 but not requiring \u2014 a client-server database on top of the library, with a single transactor and scale-out reads. Simplicity. First make it work, then make it fast. I am writing this down to remind myself to not micro-optimize. Complications should be justified by profiling measurements. Non-goals Deleting data or updating data in place. High write throughput. If you produce large volumes of data quickly, probably not all of it remains valuable for a long time, and storage space might become a concern. A time series database or durable message queue might be a better alternative. Having the fastest lookups. A key-value store may be better suited for this. Portability. Little-endian architectures that run Linux are the only target.","title":"Overview"},{"location":"#noblit","text":"Vaporware warning: much of the content below is hypothetical. Noblit is an embeddable append-only database. The database records a history of immutable (entity, attribute, value) tuples. Tuples can be asserted and retracted. A retraction is recorded as a new fact; it is not a delete. Any historical state of the database can be reproduced, and the history is first-class and queryable.","title":"Noblit"},{"location":"#features","text":"Embeddable. Noblit comes as a library that you link into your application. Global installation or daemons are not necessary. Simple but flexible data model. Noblit stores (entity, attribute, value) tuples. A traditional relational model can be expressed like this, but less rigid structures such as graphs can be expressed too. Machine-friendly relational queries. Queries are data structures rather than strings. There is no need for string formatting or escaping, and the full power of the host language is available to safely generate and compose queries. Point in time queries. Any historical state of the database can be reproduced and queried efficiently. Reified schema . Noblit stores the schema as entities in the database itself. The schema can be evolved through normal assertions and retractions, and the full migration history is available. Reified transactions . Transactions in Noblit are entities that can have attributes like any other entity. Possibilities include the transaction timestamp and the user who initiated the transaction. Transactions can be inspected in queries. Profile-guided query optimization . While queries are declarative, statement order affects the query plan. An explicit query optimizer can optimize a given query by measuring how alternatives perform, instead of having to estimate from statistics.","title":"Features"},{"location":"#comparison","text":"Noblit is heavily inspired by Datomic , especially by its data model. In comparison to other databases such as SQLite and Postgres , Noblit positions itself as follows: Client-server Embedded Mutable Postgres SQLite Immutable Datomic Noblit Noblit combines the low operational overhead of an embedded database with the simplicity of an append-only database with value semantics. In comparison to Datomic, Noblit puts more focus on static type safety. This makes schemas more rigid. On the one hand this impedes iterating quickly, but on the other hand it helps to provide data consistency in the long term. Datomic is a distributed system running on the JVM , while Noblit is a small native library. Another attempt at an embedded database inspired by Datomic was Mentat . Both Noblit and Mentat happen to be written in Rust. Mentat is a layer on top of SQLite, whereas Noblit has its own storage backend. Mentat exists but is unmaintained, Noblit is vaporware.","title":"Comparison"},{"location":"#goals","text":"An embeddable library for queries and transactions. Relational queries in a machine-first query format. Durable storage on disk, with atomic transactions. Easy to build, few or zero dependencies. Handling moderate write workloads and moderately sized data. Storing datasets that do not fit in working memory. The result of a query and data to insert must fit though. Enabling \u2014 but not requiring \u2014 a client-server database on top of the library, with a single transactor and scale-out reads. Simplicity. First make it work, then make it fast. I am writing this down to remind myself to not micro-optimize. Complications should be justified by profiling measurements.","title":"Goals"},{"location":"#non-goals","text":"Deleting data or updating data in place. High write throughput. If you produce large volumes of data quickly, probably not all of it remains valuable for a long time, and storage space might become a concern. A time series database or durable message queue might be a better alternative. Having the fastest lookups. A key-value store may be better suited for this. Portability. Little-endian architectures that run Linux are the only target.","title":"Non-goals"},{"location":"architecture/","text":"Architecture Vaporware warning: this document is as much of a roadmap, as a description of what is currently implemented. A Noblit database is conceptually a set of datoms . Indexes facilitate queries, by storing the datoms in a particular order. Indexes are trees . Tree nodes are stored in a store , and identified by a page id. The value of a datom is stored externally, on the heap . The heap is a bump-pointer backing store for values, and values are identified by their offset in the heap. As an optimization, small values can be stored inline in the datom. Immutability Noblit is immutable on two levels: Changing or deleting data is possible on a logical level, but these operations produce new datoms; datoms are immutable and they are never removed. This is similar to Git, where you can delete and change files, but changes are recorded as new commits. Indexes are implemented as persistent trees (referring to persistent data structure , not to persistent storage ). A tree modification produces a new root node, that may reference nodes from the old tree, but nodes are immutable once created, and never updated in place. Backing indexes by persistent trees, enables consistent reads to happen in parallel with writes. Once you have the root node of an index, that tree will never change. Reads based on that root will always see the same tree, providing consistency. Because the tree is immutable, parallel reads are safe, without the need for coordination. Therefore, Noblit is very suitable for read-heavy workloads. The flip side of Noblit\u2019s approach to indexing is that writes need to be serialized. A write first writes the new tree nodes, up to the new root. A commit is an atomic swap of the current root page ids. Serializing writes provides the serializable isolation level by construction, but it eliminates the possibility of trading isolation for write throughput. Therefore, Noblit is unsuitable for write-heavy workloads. Append-only Noblit is append-only on two levels: The database is conceptually a set of datoms, and the only supported operation is insert . The nodes of the index trees are stored in a page store, and new nodes get a new page id. Noblit does not reuse ids of unreachable pages. The current in-memory page store implementation is an append-only vector of pages, where the page id is simply the index. The plan for the disk-backed page store is similar: an append-only file of pages, that never reuses pages in the middle. Not reusing old pages for new tree nodes has one major advantage: caching becomes trivial. If you have retrieved the page with a given id previously, then that data will still be valid. This enables caching at all levels: caching disk data in memory, but also caching remote data locally. No coordination is required for caching either: everything is always safe to cache. An append-only page store has the additional benefit that it enables a reproducible disk format, making Noblit databases suitable for use in reproducible builds . How data ends up on disk depends only on writes. If page ids were recycled, there would need to be a coordination mechanism to determine which pages can be recycled safely, which might make the output dependent on both reads and writes. The downsides of an append-only page store are fragmentation and space use. Eventually, tree nodes will become unreachable, but they still exist in the index file, where they consume space, and form gaps between reachable nodes. Garbage collection Vaporware warning: garbage collection has not been implemented. The downsides of an append-only page store can be mitigated with a compacting garbage collection. Traverse the trees, and write all reachable nodes to a new index file. In addition to dropping unreachable nodes, the tree nodes in the new index file can be ordered such that a tree traversal becomes a sequential scan through the file, improving disk access patterns. A garbage collection can happen in parallel with normal operation. Apart from the required IO bandwith, writing a new index file does not interfere with the existing index file, which can continue to be used. If, once the collection is complete, the compacted trees are no longer the latest trees, then the missing datoms can be inserted into the new index file too (as a regular tree insertion), until the two files are in sync, at which point we can perform new writes against the new index file, and forget about the old file. TODO: To be able to efficiently get all datoms added since a given transaction though, we would need a transaction-ordered index, which Noblit currently does not have. Is it worth adding one for this purpose? Or do we do something like buffer writes while the collection is in progress? While garbage collection can mitigate the downsides of an append-only page store, it comes at a cost: During the collection, extra disk space and IO bandwidth are required. The collection causes write amplification: every datom in the database will be written again. While the hitchhiker tree itself ensures a logarithmic (in the total number of datoms) number of writes per datom, the rate at which nodes become unreachable depends on the transaction rate in addition to the total number of datoms, so especially in the case of many small transactions, garbage collection may be required often. Coordination is required to know when the old index file can safely be removed, after collection is complete. The page id no longer uniquely identifies a tree node. To retain the benefits of not reusing old names for new pages, we would need to identify tree nodes by the version of the index file, in addition to their page id. As an alternative to garbage collection, it may be possible to keep most of the benefits of an append-only page store, while avoiding the wasted space of backing it by an append-only file, by keeping track of a mutable mapping from (virtual) page ids to (physical) file offsets. The downside is that coordination is required to update this mapping, even for reads. Also, while this avoids wasted space, it does not by itself mitigate fragmentation.","title":"Architecture"},{"location":"architecture/#architecture","text":"Vaporware warning: this document is as much of a roadmap, as a description of what is currently implemented. A Noblit database is conceptually a set of datoms . Indexes facilitate queries, by storing the datoms in a particular order. Indexes are trees . Tree nodes are stored in a store , and identified by a page id. The value of a datom is stored externally, on the heap . The heap is a bump-pointer backing store for values, and values are identified by their offset in the heap. As an optimization, small values can be stored inline in the datom.","title":"Architecture"},{"location":"architecture/#immutability","text":"Noblit is immutable on two levels: Changing or deleting data is possible on a logical level, but these operations produce new datoms; datoms are immutable and they are never removed. This is similar to Git, where you can delete and change files, but changes are recorded as new commits. Indexes are implemented as persistent trees (referring to persistent data structure , not to persistent storage ). A tree modification produces a new root node, that may reference nodes from the old tree, but nodes are immutable once created, and never updated in place. Backing indexes by persistent trees, enables consistent reads to happen in parallel with writes. Once you have the root node of an index, that tree will never change. Reads based on that root will always see the same tree, providing consistency. Because the tree is immutable, parallel reads are safe, without the need for coordination. Therefore, Noblit is very suitable for read-heavy workloads. The flip side of Noblit\u2019s approach to indexing is that writes need to be serialized. A write first writes the new tree nodes, up to the new root. A commit is an atomic swap of the current root page ids. Serializing writes provides the serializable isolation level by construction, but it eliminates the possibility of trading isolation for write throughput. Therefore, Noblit is unsuitable for write-heavy workloads.","title":"Immutability"},{"location":"architecture/#append-only","text":"Noblit is append-only on two levels: The database is conceptually a set of datoms, and the only supported operation is insert . The nodes of the index trees are stored in a page store, and new nodes get a new page id. Noblit does not reuse ids of unreachable pages. The current in-memory page store implementation is an append-only vector of pages, where the page id is simply the index. The plan for the disk-backed page store is similar: an append-only file of pages, that never reuses pages in the middle. Not reusing old pages for new tree nodes has one major advantage: caching becomes trivial. If you have retrieved the page with a given id previously, then that data will still be valid. This enables caching at all levels: caching disk data in memory, but also caching remote data locally. No coordination is required for caching either: everything is always safe to cache. An append-only page store has the additional benefit that it enables a reproducible disk format, making Noblit databases suitable for use in reproducible builds . How data ends up on disk depends only on writes. If page ids were recycled, there would need to be a coordination mechanism to determine which pages can be recycled safely, which might make the output dependent on both reads and writes. The downsides of an append-only page store are fragmentation and space use. Eventually, tree nodes will become unreachable, but they still exist in the index file, where they consume space, and form gaps between reachable nodes.","title":"Append-only"},{"location":"architecture/#garbage-collection","text":"Vaporware warning: garbage collection has not been implemented. The downsides of an append-only page store can be mitigated with a compacting garbage collection. Traverse the trees, and write all reachable nodes to a new index file. In addition to dropping unreachable nodes, the tree nodes in the new index file can be ordered such that a tree traversal becomes a sequential scan through the file, improving disk access patterns. A garbage collection can happen in parallel with normal operation. Apart from the required IO bandwith, writing a new index file does not interfere with the existing index file, which can continue to be used. If, once the collection is complete, the compacted trees are no longer the latest trees, then the missing datoms can be inserted into the new index file too (as a regular tree insertion), until the two files are in sync, at which point we can perform new writes against the new index file, and forget about the old file. TODO: To be able to efficiently get all datoms added since a given transaction though, we would need a transaction-ordered index, which Noblit currently does not have. Is it worth adding one for this purpose? Or do we do something like buffer writes while the collection is in progress? While garbage collection can mitigate the downsides of an append-only page store, it comes at a cost: During the collection, extra disk space and IO bandwidth are required. The collection causes write amplification: every datom in the database will be written again. While the hitchhiker tree itself ensures a logarithmic (in the total number of datoms) number of writes per datom, the rate at which nodes become unreachable depends on the transaction rate in addition to the total number of datoms, so especially in the case of many small transactions, garbage collection may be required often. Coordination is required to know when the old index file can safely be removed, after collection is complete. The page id no longer uniquely identifies a tree node. To retain the benefits of not reusing old names for new pages, we would need to identify tree nodes by the version of the index file, in addition to their page id. As an alternative to garbage collection, it may be possible to keep most of the benefits of an append-only page store, while avoiding the wasted space of backing it by an append-only file, by keeping track of a mutable mapping from (virtual) page ids to (physical) file offsets. The downside is that coordination is required to update this mapping, even for reads. Also, while this avoids wasted space, it does not by itself mitigate fragmentation.","title":"Garbage collection"},{"location":"building/","text":"Building Noblit is a native library written in Rust. It will come with official client libraries for Haskell, Python, and Rust. The Haskell and Python client are under construction; the Rust client does not yet exist, but the Noblit crate itself can be used. Build tools Nix can set up a build environment in which all required build tools are available. The repository contains a default.nix file that defines the environment. All build tools are pinned for reproducibility. The Nix environment is used for CI , so it is actively tested. There are three ways to use the build environment: Enter a shell in which all build tools are available with nix run -c $SHELL . Prefix all commands with nix run -c . Bring the binaries into your PATH with export PATH=$(nix-build --no-out-link)/bin:$PATH . Using Nix is convenient, but not a requirement. You can source your build tools elsewhere if you like. Noblit Noblit builds with Rust\u2019s build tool Cargo. Noblit is developed and tested against Rust 1.28.0, because this was the Rust version that the latest two Ubuntu LTSes as well as Debian Testing shipped at the time of its inception. Later versions of Rust may work. Noblit has no dependencies apart from the Rust standard library. To build: $ cargo build --release $ ls target/release/libnoblit* This will have produced three libraries: libnoblit.so , for dynamic linking against the C interface. libnoblit.a , for static linking against the C interface. libnoblit.rlib , for use in Rust programs. If you need a header file, a script can generate one from the C interface documentation : $ tools/gen_header.py > noblit.h Haskell client The Haskell client is located in client/haskell and builds with Stack. Currently the library supports Stackage LTS 13 (GHC 8.6). To build: $ cd client/haskell $ stack setup $ stack build TODO: How does it find the Rust lib? TODO: How to use in an application? Python client The Python client is located in client/python . It loads libnoblit.so using Python\u2019s ctypes module. If loading fails for the unqualified path, the library tries to load from target/debug and target/release to aid local development. Python code does not need to be compiled, but it can be typechecked by Mypy : $ mypy --strict client/python Rust client Rust programs can use the noblit crate in the noblit directory directly, although that exposes internals, and building queries is neithere convenient nor type safe. It would be nice to add a layer on top that hides most of the internals and exposes a more user-focused interface, but such a Rust client does not yet exist.","title":"Building"},{"location":"building/#building","text":"Noblit is a native library written in Rust. It will come with official client libraries for Haskell, Python, and Rust. The Haskell and Python client are under construction; the Rust client does not yet exist, but the Noblit crate itself can be used.","title":"Building"},{"location":"building/#build-tools","text":"Nix can set up a build environment in which all required build tools are available. The repository contains a default.nix file that defines the environment. All build tools are pinned for reproducibility. The Nix environment is used for CI , so it is actively tested. There are three ways to use the build environment: Enter a shell in which all build tools are available with nix run -c $SHELL . Prefix all commands with nix run -c . Bring the binaries into your PATH with export PATH=$(nix-build --no-out-link)/bin:$PATH . Using Nix is convenient, but not a requirement. You can source your build tools elsewhere if you like.","title":"Build tools"},{"location":"building/#noblit","text":"Noblit builds with Rust\u2019s build tool Cargo. Noblit is developed and tested against Rust 1.28.0, because this was the Rust version that the latest two Ubuntu LTSes as well as Debian Testing shipped at the time of its inception. Later versions of Rust may work. Noblit has no dependencies apart from the Rust standard library. To build: $ cargo build --release $ ls target/release/libnoblit* This will have produced three libraries: libnoblit.so , for dynamic linking against the C interface. libnoblit.a , for static linking against the C interface. libnoblit.rlib , for use in Rust programs. If you need a header file, a script can generate one from the C interface documentation : $ tools/gen_header.py > noblit.h","title":"Noblit"},{"location":"building/#haskell-client","text":"The Haskell client is located in client/haskell and builds with Stack. Currently the library supports Stackage LTS 13 (GHC 8.6). To build: $ cd client/haskell $ stack setup $ stack build TODO: How does it find the Rust lib? TODO: How to use in an application?","title":"Haskell client"},{"location":"building/#python-client","text":"The Python client is located in client/python . It loads libnoblit.so using Python\u2019s ctypes module. If loading fails for the unqualified path, the library tries to load from target/debug and target/release to aid local development. Python code does not need to be compiled, but it can be typechecked by Mypy : $ mypy --strict client/python","title":"Python client"},{"location":"building/#rust-client","text":"Rust programs can use the noblit crate in the noblit directory directly, although that exposes internals, and building queries is neithere convenient nor type safe. It would be nice to add a layer on top that hides most of the internals and exposes a more user-focused interface, but such a Rust client does not yet exist.","title":"Rust client"},{"location":"data-model/","text":"Data model Noblit stores datoms . Datoms assert or retract attributes of entities. For example, entity 12 might have an attribute user.email with value rachael@tyrell.com . Conceptually, the database is an append-only log of (entity, attribute, value) tuples, together with the time at which they were asserted or retracted in the form of a transaction id. A view of the database at a given point in time is the set of all (entity, attribute, value) tuples that have been asserted and not retracted before or at that time. A datom is a tuple of the following five values: Entity : An integer that uniquely identifies an entity in the database. Attribute : Analogous to a column in a relational database. Value : The value for the attribute. Transaction : The transaction id of the transaction that added the datom. Operation : Either assert or retract . To get a view of the database at a given transaction t , we exclude all datoms with a transaction greater than t . Then we cancel assertions against subsequent retractions. What is left is the set of (entity, attribute, value) tuples that were true after transaction t .","title":"Data model"},{"location":"data-model/#data-model","text":"Noblit stores datoms . Datoms assert or retract attributes of entities. For example, entity 12 might have an attribute user.email with value rachael@tyrell.com . Conceptually, the database is an append-only log of (entity, attribute, value) tuples, together with the time at which they were asserted or retracted in the form of a transaction id. A view of the database at a given point in time is the set of all (entity, attribute, value) tuples that have been asserted and not retracted before or at that time. A datom is a tuple of the following five values: Entity : An integer that uniquely identifies an entity in the database. Attribute : Analogous to a column in a relational database. Value : The value for the attribute. Transaction : The transaction id of the transaction that added the datom. Operation : Either assert or retract . To get a view of the database at a given transaction t , we exclude all datoms with a transaction greater than t . Then we cancel assertions against subsequent retractions. What is left is the set of (entity, attribute, value) tuples that were true after transaction t .","title":"Data model"},{"location":"distributed-operation/","text":"Distributed operation Noblit is a modular database library. It consists of several components: The page store and heap, which store index tree nodes and large datom values. A query engine, that requires only read access to the store and heap. A mutator responsible for processing transactions. The mutator requires write access to the store and heap. These components are part of a single native dynamic library that you can link into your application. The store and the heap are an interface ( trait in Rust) with several implementations. Noblit features an in-memory store and heap, and a file-backed store and heap. The modular approach allows Noblit to be used in a few different ways. Vaporware warning: the following paragraphs about distributed operation are an idea for the distant future. While Noblit is designed with distributed operation in mind, none of it has been implemented. Local operation With its memory-backed and file-backed store, Noblit is similar to SQLite : an embeddable native library for interacting with an in-memory database, or a local file-backed database. Deconstructing the database On top of Noblit the library, we can build a daemon that exposes query and mutations over the network. Unlike traditional relational databases, the server would not need to be a monolith. Like Datomic , Noblit could be deployed as a deconstructed database , where storage, query processing, and mutation, do not need to reside in the same place. Query processing can be done at clients through the embedded library, same as in local operation. This is similar to Datomic\u2019s peer library. Alternatively, query processing can happen at a dedicated server, in the traditional client-server model. Regardless of where query processing happens, decoupling query from storage allows scaling reads horizontally. Distributed reads Query processing requires an implementation of a store and heap, readable, but not necessarily writable. Because these store immutable data, they can be scaled easily. The store and heap may be backed by a remote service, rather than by a local file. Because everything is immutable, all data can be cached safely at every level. A multi-level cache is possible. Mix and match a local memory cache, a distributed memory cache (like Memcached), a disk cache backed by a fast but volatile local SSD , and a cache backed by slow but stable storage. Check peers first, and only hit the main source of truth (the store and heap that the mutator writes to) if the page is not available elsewhere. This reduces load on the mutator, so it can devote all its resources to transaction processing. When clients execute a query, they need to specify the revision of the database to execute it agaist. The current latest revision is the only piece of mutable state in Noblit, and the mutator is responsible for managing it. Committing a transaction causes that transaction to become the new latest revision. Often, clients want to execute a query against the lastest revision of the database. To get the latest revision, they would need to query the mutator, making the mutator contended again, even for reads. Obtaining the latest revision is inherently racy: by the time the client receives the response, it may already be outdated. Fortunately, this is often not a problem; clients do not want the absolute latest revision, just a sufficiently recent revision. This means that replicas can also service the \u201cget latest\u201d query, as long as they do not lag too far behind. Together, this means that query capacity can be scaled virtually without limit. The cost of adding a read replica \u2014 the bandwith of reading pages that have not yet been cached locally from its peers \u2014 can be amortized over all existing peers, so it goes down per added replica. As long as stale (but consistent) data is acceptable, this makes it easy to make reads highly available. Distributed writes Noblit serializes writes. The easiest way to do that, is by executing them on a single thread on a single machine. This makes it impossible to scale writes horizontally. For availability reasons \u2014 not for performance \u2014 it may be desirable to replace the single mutator with a multi-node mutator that runs a distributed consensus algorithm to serialize mutations. However, for availability, it is probably a easier to have a stand-by mutator and a way to switch, than it is to build a distributed system.","title":"Distributed operation"},{"location":"distributed-operation/#distributed-operation","text":"Noblit is a modular database library. It consists of several components: The page store and heap, which store index tree nodes and large datom values. A query engine, that requires only read access to the store and heap. A mutator responsible for processing transactions. The mutator requires write access to the store and heap. These components are part of a single native dynamic library that you can link into your application. The store and the heap are an interface ( trait in Rust) with several implementations. Noblit features an in-memory store and heap, and a file-backed store and heap. The modular approach allows Noblit to be used in a few different ways. Vaporware warning: the following paragraphs about distributed operation are an idea for the distant future. While Noblit is designed with distributed operation in mind, none of it has been implemented.","title":"Distributed operation"},{"location":"distributed-operation/#local-operation","text":"With its memory-backed and file-backed store, Noblit is similar to SQLite : an embeddable native library for interacting with an in-memory database, or a local file-backed database.","title":"Local operation"},{"location":"distributed-operation/#deconstructing-the-database","text":"On top of Noblit the library, we can build a daemon that exposes query and mutations over the network. Unlike traditional relational databases, the server would not need to be a monolith. Like Datomic , Noblit could be deployed as a deconstructed database , where storage, query processing, and mutation, do not need to reside in the same place. Query processing can be done at clients through the embedded library, same as in local operation. This is similar to Datomic\u2019s peer library. Alternatively, query processing can happen at a dedicated server, in the traditional client-server model. Regardless of where query processing happens, decoupling query from storage allows scaling reads horizontally.","title":"Deconstructing the database"},{"location":"distributed-operation/#distributed-reads","text":"Query processing requires an implementation of a store and heap, readable, but not necessarily writable. Because these store immutable data, they can be scaled easily. The store and heap may be backed by a remote service, rather than by a local file. Because everything is immutable, all data can be cached safely at every level. A multi-level cache is possible. Mix and match a local memory cache, a distributed memory cache (like Memcached), a disk cache backed by a fast but volatile local SSD , and a cache backed by slow but stable storage. Check peers first, and only hit the main source of truth (the store and heap that the mutator writes to) if the page is not available elsewhere. This reduces load on the mutator, so it can devote all its resources to transaction processing. When clients execute a query, they need to specify the revision of the database to execute it agaist. The current latest revision is the only piece of mutable state in Noblit, and the mutator is responsible for managing it. Committing a transaction causes that transaction to become the new latest revision. Often, clients want to execute a query against the lastest revision of the database. To get the latest revision, they would need to query the mutator, making the mutator contended again, even for reads. Obtaining the latest revision is inherently racy: by the time the client receives the response, it may already be outdated. Fortunately, this is often not a problem; clients do not want the absolute latest revision, just a sufficiently recent revision. This means that replicas can also service the \u201cget latest\u201d query, as long as they do not lag too far behind. Together, this means that query capacity can be scaled virtually without limit. The cost of adding a read replica \u2014 the bandwith of reading pages that have not yet been cached locally from its peers \u2014 can be amortized over all existing peers, so it goes down per added replica. As long as stale (but consistent) data is acceptable, this makes it easy to make reads highly available.","title":"Distributed reads"},{"location":"distributed-operation/#distributed-writes","text":"Noblit serializes writes. The easiest way to do that, is by executing them on a single thread on a single machine. This makes it impossible to scale writes horizontally. For availability reasons \u2014 not for performance \u2014 it may be desirable to replace the single mutator with a multi-node mutator that runs a distributed consensus algorithm to serialize mutations. However, for availability, it is probably a easier to have a stand-by mutator and a way to switch, than it is to build a distributed system.","title":"Distributed writes"},{"location":"execution/","text":"Execution This page gives a high-level overview of how queries are executed. Rust\u2019s borrowing rules enforce a rigid structure on the ordering of operations. The components involved are: Indexes : The index trees and their backing storage. Because indexes are persistent data structures, it would be possible to have multiple readers and a single writer simultaneously. However, the current implementation requires exclusive access for writes, as this is easier to express in Rust. The heap : Large values live on the large value heap. Like indexes it is append-only, so it would admit multiple readers and a single writer simultaneously, but the current implementation requires exclusive access. The temporary heap : Query evaluation requires all large values to live on a heap. Large values that occur in queries are placed on a temporary heap. The temporary heap is a layer on top of the persistent large value heap: lookups for non-temporaries fall through to the underlying heap. Queries Create a temporary heap. Parse the query. Place values on the temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query. Release persistent indexes and heap (read-only). Drop the temporaries. Mutations Create a temporary heap. (TODO: Create two, one for reads, one for writes.) Parse the query. Place values that occur in the query part on the reads temporary heap. Place values that occur in assertions on the writes temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query part (if any, otherwise use unit). For every result, collect new datoms. Release persistent indexes and heap (read-only). Persist any temporaries referenced by new datoms. This requires write access to the persistent heap. Acquire the persistent heap, now containing necessary values (read-only). Persist new datoms. This requires write access to the persistent indexes. Release the persistent heap (read-only).","title":"Execution"},{"location":"execution/#execution","text":"This page gives a high-level overview of how queries are executed. Rust\u2019s borrowing rules enforce a rigid structure on the ordering of operations. The components involved are: Indexes : The index trees and their backing storage. Because indexes are persistent data structures, it would be possible to have multiple readers and a single writer simultaneously. However, the current implementation requires exclusive access for writes, as this is easier to express in Rust. The heap : Large values live on the large value heap. Like indexes it is append-only, so it would admit multiple readers and a single writer simultaneously, but the current implementation requires exclusive access. The temporary heap : Query evaluation requires all large values to live on a heap. Large values that occur in queries are placed on a temporary heap. The temporary heap is a layer on top of the persistent large value heap: lookups for non-temporaries fall through to the underlying heap.","title":"Execution"},{"location":"execution/#queries","text":"Create a temporary heap. Parse the query. Place values on the temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query. Release persistent indexes and heap (read-only). Drop the temporaries.","title":"Queries"},{"location":"execution/#mutations","text":"Create a temporary heap. (TODO: Create two, one for reads, one for writes.) Parse the query. Place values that occur in the query part on the reads temporary heap. Place values that occur in assertions on the writes temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query part (if any, otherwise use unit). For every result, collect new datoms. Release persistent indexes and heap (read-only). Persist any temporaries referenced by new datoms. This requires write access to the persistent heap. Acquire the persistent heap, now containing necessary values (read-only). Persist new datoms. This requires write access to the persistent indexes. Release the persistent heap (read-only).","title":"Mutations"},{"location":"files/","text":"Files Vaporware warning: much of the content below is hypothetical. Currently Noblit does not persist anything to disk at all. Noblit stores thee kinds of data on disk: The heap of large values (integers larger than 62 bits, and byte strings longer than 7 bytes). The indexes that store datoms in sorted order. The head with the most recent tree roots, and id counters. Accretion Because Noblit is an append-only database, transactions only add new datoms. Datoms are only stored in indexes (the indexes are covering ), but they may reference large values on the heap. Indexes are trees that consist of nodes. The indexes are persistent data structures, in the sense that data is immutable once written, but we can construct a new index that shares most of its nodes with a previous version. The index file is an append-only collection of nodes. The head points to the latest roots of the index trees, and it stores the counters for allocating entity ids. The head is the only part of Noblit that is updated in place, the other files are append-only. Committing a transaction involves three writes and a sync: Append any new large values to the heap file. Add datoms to the indexes. This produces one or more new tree nodes per index. Append the new nodes to the index file. Write the new head. Sync the heap file, the index file, and finally the head. By making the head update atomic, the entire transaction becomes atomic. If the commit fails at some point before the new head is written, the old head is still valid, and it points to valid data. New data may have been appended, but that data is not yet referenced. The Indexes The indexes in Noblit store datoms in sorted order. There are three indexes: Eavt : sorted by entity, attribute, value, and transaction. Aevt : sorted by attribute, entity, value, and transaction. Avet : sorted by attribute, value, entity, and transaction. See also Datomic's index documentation , which formed the inspiration for Noblit. Note that unlike Datomic, Noblit does not have a Vaet (value, attribute, entity, transaction) index. This is because attributes in Noblit are strongly typed. A query such as \u201clist all attributes that this entity has\u201d is impossible to express in Noblit, because the values associated with the attribute may not have the same type. Each index stores all datoms in full. A datom is 32 bytes. Small values are stored inline in the datom, large values (integers larger than 62 bits and byte strings longer than 7 bytes) are stored on the value heap, with the datom containing the index into the value heap. Noblit stores indexes as hitchhiker trees , a variation on immutable B-trees which reduces write amplification. Noblit accumulates new datoms in memory first. At the end of a transaction it flushes those at once to the disk as new tree nodes, which may share child nodes with the previous tree. To prevent unreachable nodes from accumulating, trees need to be compacted occasionally though a copying garbage collection process. Unreachable nodes are not recycled to ensure immutability of written files: new data is only ever appended at the end. This simplifies caching. The Head For every index, the head stores the page id of the root of the tree for that index. Furthermore, the head stores the counters for id allocation. TODO: The head should store the size of the index file and heap file, so that it can truncate them after a failed transaction. The Heap The heap is a file that contains large values : integers larger than 62 bytes and byte strings longer than 7 bytes. Because Noblit is an append-only database that never removes data, storing a value on the heap is a simple bump-pointer allocation; the heap only grows. The heap stores two kinds of values: 64-bit integers which don't fit into 62 bytes. They are stored as-is, in 8 bytes, big endian. (TODO: It should be little endian.) Byte strings longer than 7 bytes. Note that e.g. strings are also stored as byte strings; it is the schema that specifies that those bytes should be interpreted as a UTF-8 encoded string. Byte strings are length-prefixed with a 64-bit length. The address of a byte string is the offset of its length prefix, so its data can be found at the offset 8 bytes higher. Values on the value heap are aligned to 8 bytes. The heap is not checksummed. (Nor the indexes for that matter.) If you do not trust your storage medium, use a file system or virtual block device that can detect and report integrity problems. The value heap might store duplicates. Because values are immutable once stored, deduplication is safe. If a Datom contains a value that already exists on the heap, it is safe to reference the existing value, rather than storing it again on the heap. Noblit may do this, but identifying duplicates is not free, hence Noblit may store the same value twice. TODO: Persist a hash table of values too, to identify duplicates? Not really, can read heap at startup, although that does not scale, persistent hash table may be needed.","title":"Files"},{"location":"files/#files","text":"Vaporware warning: much of the content below is hypothetical. Currently Noblit does not persist anything to disk at all. Noblit stores thee kinds of data on disk: The heap of large values (integers larger than 62 bits, and byte strings longer than 7 bytes). The indexes that store datoms in sorted order. The head with the most recent tree roots, and id counters.","title":"Files"},{"location":"files/#accretion","text":"Because Noblit is an append-only database, transactions only add new datoms. Datoms are only stored in indexes (the indexes are covering ), but they may reference large values on the heap. Indexes are trees that consist of nodes. The indexes are persistent data structures, in the sense that data is immutable once written, but we can construct a new index that shares most of its nodes with a previous version. The index file is an append-only collection of nodes. The head points to the latest roots of the index trees, and it stores the counters for allocating entity ids. The head is the only part of Noblit that is updated in place, the other files are append-only. Committing a transaction involves three writes and a sync: Append any new large values to the heap file. Add datoms to the indexes. This produces one or more new tree nodes per index. Append the new nodes to the index file. Write the new head. Sync the heap file, the index file, and finally the head. By making the head update atomic, the entire transaction becomes atomic. If the commit fails at some point before the new head is written, the old head is still valid, and it points to valid data. New data may have been appended, but that data is not yet referenced.","title":"Accretion"},{"location":"files/#the-indexes","text":"The indexes in Noblit store datoms in sorted order. There are three indexes: Eavt : sorted by entity, attribute, value, and transaction. Aevt : sorted by attribute, entity, value, and transaction. Avet : sorted by attribute, value, entity, and transaction. See also Datomic's index documentation , which formed the inspiration for Noblit. Note that unlike Datomic, Noblit does not have a Vaet (value, attribute, entity, transaction) index. This is because attributes in Noblit are strongly typed. A query such as \u201clist all attributes that this entity has\u201d is impossible to express in Noblit, because the values associated with the attribute may not have the same type. Each index stores all datoms in full. A datom is 32 bytes. Small values are stored inline in the datom, large values (integers larger than 62 bits and byte strings longer than 7 bytes) are stored on the value heap, with the datom containing the index into the value heap. Noblit stores indexes as hitchhiker trees , a variation on immutable B-trees which reduces write amplification. Noblit accumulates new datoms in memory first. At the end of a transaction it flushes those at once to the disk as new tree nodes, which may share child nodes with the previous tree. To prevent unreachable nodes from accumulating, trees need to be compacted occasionally though a copying garbage collection process. Unreachable nodes are not recycled to ensure immutability of written files: new data is only ever appended at the end. This simplifies caching.","title":"The Indexes"},{"location":"files/#the-head","text":"For every index, the head stores the page id of the root of the tree for that index. Furthermore, the head stores the counters for id allocation. TODO: The head should store the size of the index file and heap file, so that it can truncate them after a failed transaction.","title":"The Head"},{"location":"files/#the-heap","text":"The heap is a file that contains large values : integers larger than 62 bytes and byte strings longer than 7 bytes. Because Noblit is an append-only database that never removes data, storing a value on the heap is a simple bump-pointer allocation; the heap only grows. The heap stores two kinds of values: 64-bit integers which don't fit into 62 bytes. They are stored as-is, in 8 bytes, big endian. (TODO: It should be little endian.) Byte strings longer than 7 bytes. Note that e.g. strings are also stored as byte strings; it is the schema that specifies that those bytes should be interpreted as a UTF-8 encoded string. Byte strings are length-prefixed with a 64-bit length. The address of a byte string is the offset of its length prefix, so its data can be found at the offset 8 bytes higher. Values on the value heap are aligned to 8 bytes. The heap is not checksummed. (Nor the indexes for that matter.) If you do not trust your storage medium, use a file system or virtual block device that can detect and report integrity problems. The value heap might store duplicates. Because values are immutable once stored, deduplication is safe. If a Datom contains a value that already exists on the heap, it is safe to reference the existing value, rather than storing it again on the heap. Noblit may do this, but identifying duplicates is not free, hence Noblit may store the same value twice. TODO: Persist a hash table of values too, to identify duplicates? Not really, can read heap at startup, although that does not scale, persistent hash table may be needed.","title":"The Heap"},{"location":"fuzz-tests/","text":"Fuzz tests Noblits internals are tested thoroughly through fuzz testing. Noblit uses structure-aware fuzzing to generate sequences of calls to internal API s, after which all invariants are checked. See the code in the fuzz directory. TODO: Expand these docs. TODO: Add more traditional fuzzer, for the parser of the binary query format. TODO: Add a structure-aware fuzzer for the higher-level API , in addition to those that focus on internals. Maybe just against the C interface. TODO: Add an abstraction for IO , and an implementation that that can inject errors based on the fuzz input.","title":"Fuzz tests"},{"location":"fuzz-tests/#fuzz-tests","text":"Noblits internals are tested thoroughly through fuzz testing. Noblit uses structure-aware fuzzing to generate sequences of calls to internal API s, after which all invariants are checked. See the code in the fuzz directory. TODO: Expand these docs. TODO: Add more traditional fuzzer, for the parser of the binary query format. TODO: Add a structure-aware fuzzer for the higher-level API , in addition to those that focus on internals. Maybe just against the C interface. TODO: Add an abstraction for IO , and an implementation that that can inject errors based on the fuzz input.","title":"Fuzz tests"},{"location":"golden-tests/","text":"Golden tests Noblit comes with a suite of golden test: queries with known-good reference results. The golden tests are stored in .t files in the golden directory. Each file contains a query and its expected outcome. Goldens can be verified with the test runner, golden/run.py . Example Below is a formatted example golden test that selects the entity id and name of every built-in type. It consists of the query, followed by the expected output: where t db.type.name name select t, name t name # 7 db.type.bool # 10 db.type.bytes # 8 db.type.ref # 11 db.type.string # 9 db.type.uint64 The above example is formatted for inclusion in the docs. In the .t files, expected output is included as a table drawn with box-drawing characters, as this is also what the execute binary prints. The encoding of the .t file is UTF-8 . Checking all goldens The test runner golden/run.py is desiged to be used with a TAP harness such as Prove . The runner prints TAP -compliant output to stdout. The runner can be used with Prove to verify all goldens: $ prove --exec golden/run.py golden This will run golden/run.py for every .t file in the golden directory, and print a summary of the results. Prove looks for .t files by default, which is also why Noblit uses that extension for goldens. You can get verbose output of the query and output by passing --verbose : $ prove --exec 'golden/run.py --verbose' --verbose golden Stages Checking a golden consists of several stages: The runner reads .t file and splits it into a query and expected outcome. The query is parsed by golden/parse.py , and serialized into a binary format. The execute binary parses the binary query and executes it. The runner pipes the binary query into the execute binary, and compares its output with the reference output. The parser and executor can also be used standalone. For example, to run the builtin types query: $ cat golden/builtin_types.t | golden/parse.py | target/debug/execute In this mode, the parser discards the reference output in the file.","title":"Golden tests"},{"location":"golden-tests/#golden-tests","text":"Noblit comes with a suite of golden test: queries with known-good reference results. The golden tests are stored in .t files in the golden directory. Each file contains a query and its expected outcome. Goldens can be verified with the test runner, golden/run.py .","title":"Golden tests"},{"location":"golden-tests/#example","text":"Below is a formatted example golden test that selects the entity id and name of every built-in type. It consists of the query, followed by the expected output: where t db.type.name name select t, name t name # 7 db.type.bool # 10 db.type.bytes # 8 db.type.ref # 11 db.type.string # 9 db.type.uint64 The above example is formatted for inclusion in the docs. In the .t files, expected output is included as a table drawn with box-drawing characters, as this is also what the execute binary prints. The encoding of the .t file is UTF-8 .","title":"Example"},{"location":"golden-tests/#checking-all-goldens","text":"The test runner golden/run.py is desiged to be used with a TAP harness such as Prove . The runner prints TAP -compliant output to stdout. The runner can be used with Prove to verify all goldens: $ prove --exec golden/run.py golden This will run golden/run.py for every .t file in the golden directory, and print a summary of the results. Prove looks for .t files by default, which is also why Noblit uses that extension for goldens. You can get verbose output of the query and output by passing --verbose : $ prove --exec 'golden/run.py --verbose' --verbose golden","title":"Checking all goldens"},{"location":"golden-tests/#stages","text":"Checking a golden consists of several stages: The runner reads .t file and splits it into a query and expected outcome. The query is parsed by golden/parse.py , and serialized into a binary format. The execute binary parses the binary query and executes it. The runner pipes the binary query into the execute binary, and compares its output with the reference output. The parser and executor can also be used standalone. For example, to run the builtin types query: $ cat golden/builtin_types.t | golden/parse.py | target/debug/execute In this mode, the parser discards the reference output in the file.","title":"Stages"},{"location":"htree/","text":"Trees Indexes in Noblit are hitchhiker trees . The trees have the same on-disk format as memory format, which allows them to be memory mapped. A hitchhiker tree is similar to an immutable B-tree, but updates usually only allocate a single new node, rather than log B (n) nodes. This gives us the advantages of an immutable data structure, without the write amplification. Tree nodes in Noblit are immutable. Index updates produce new nodes that succeed older nodes. Nodes can become unreachable, but they are never removed. Eventually, many nodes in a file may not be reachable. In that case the tree can be copied to a new file, omitting the unreachable nodes. This is a copying garbage collection. Index Trees Indexes in Noblit are sorted sets of datoms. Each datom is 32 bytes. (For large values, the datom contains a reference to a value on the heap.) The indexes store the datoms themselves: they are sorted sets, not key-values maps. In other words, indexes are covering indexes . Trees in Noblit store every datom exactly once. Datoms in leaves are not repeated in interior nodes, unlike a B+ tree, which would store all datoms in leaf nodes, and repeat some datoms as midpoints in interior nodes. In addition to the midpoint datoms, tree nodes store pending datoms: datoms that need to be flushed into the leaves, but which we avoid as long as possible. This modification is what turns a B-tree into a hitchhiker tree. Disk Format Tree nodes are 4096 bytes. Byte 4088 through 4095 contains the node header, which is built up of the following 8 bytes: Byte 0 contains the depth of the node (0 for a leaf, 1 for its parent, etc.). Byte 1 contains the number of datoms in the node internally, say k . k is at most 102. TODO: Leaf nodes could store 127 datoms and no child page ids, as opposed to 102 midpoints. The remaining 6 bytes are currently not used. TODO: I could store a checksum there. At byte 0, the datom array starts. It contains k datoms in increasing order. At byte 3264, the child array starts. It contains 64-bit page ids of the child nodes. The child array has k + 1 elements, such that all datoms in node children[i] order before datoms[i] . All datoms in node children[i + 1] order after datoms[i] . The special value 2 64 \u2013 1 indicates that there is no child node in this slot. If this is the case for slot i , then datom i is a pending datom rather than a midpoint, and that datom should be flushed to a child node if space runs out in the node.","title":"Trees"},{"location":"htree/#trees","text":"Indexes in Noblit are hitchhiker trees . The trees have the same on-disk format as memory format, which allows them to be memory mapped. A hitchhiker tree is similar to an immutable B-tree, but updates usually only allocate a single new node, rather than log B (n) nodes. This gives us the advantages of an immutable data structure, without the write amplification. Tree nodes in Noblit are immutable. Index updates produce new nodes that succeed older nodes. Nodes can become unreachable, but they are never removed. Eventually, many nodes in a file may not be reachable. In that case the tree can be copied to a new file, omitting the unreachable nodes. This is a copying garbage collection.","title":"Trees"},{"location":"htree/#index-trees","text":"Indexes in Noblit are sorted sets of datoms. Each datom is 32 bytes. (For large values, the datom contains a reference to a value on the heap.) The indexes store the datoms themselves: they are sorted sets, not key-values maps. In other words, indexes are covering indexes . Trees in Noblit store every datom exactly once. Datoms in leaves are not repeated in interior nodes, unlike a B+ tree, which would store all datoms in leaf nodes, and repeat some datoms as midpoints in interior nodes. In addition to the midpoint datoms, tree nodes store pending datoms: datoms that need to be flushed into the leaves, but which we avoid as long as possible. This modification is what turns a B-tree into a hitchhiker tree.","title":"Index Trees"},{"location":"htree/#disk-format","text":"Tree nodes are 4096 bytes. Byte 4088 through 4095 contains the node header, which is built up of the following 8 bytes: Byte 0 contains the depth of the node (0 for a leaf, 1 for its parent, etc.). Byte 1 contains the number of datoms in the node internally, say k . k is at most 102. TODO: Leaf nodes could store 127 datoms and no child page ids, as opposed to 102 midpoints. The remaining 6 bytes are currently not used. TODO: I could store a checksum there. At byte 0, the datom array starts. It contains k datoms in increasing order. At byte 3264, the child array starts. It contains 64-bit page ids of the child nodes. The child array has k + 1 elements, such that all datoms in node children[i] order before datoms[i] . All datoms in node children[i + 1] order after datoms[i] . The special value 2 64 \u2013 1 indicates that there is no child node in this slot. If this is the case for slot i , then datom i is a pending datom rather than a midpoint, and that datom should be flushed to a child node if space runs out in the node.","title":"Disk Format"},{"location":"query-optimization/","text":"Query optimization The order of statements in a query affect the query plan that Noblit chooses. As outlined in the Query planning chapter, every statement translates to a loop that scans over an index. The planner preserves statement order: the first statement becomes the outer loop, the last statement becomes the inner loop. Therefore, the statement order of the query can have a big effect on query performance. To help optimize the order of statements, Noblit features a query optimizer. The optimizer is not part of the query planner. It is a standalone function that takes a query and a database, and returns the optimized query. There are two ways to optimize a query plan: Optimizing statement order , also called macro-optimization , because statement order can make orders of magnitude difference in the evaluation time of a query. This is because statement order can make a difference in complexity, it can be the difference between a query plan that is quadratic in the size of an index, versus a query plan that is constant time. Statement order can be the difference between microseconds, and minutes. Choosing the indexes to use for a scan , also called micro-optimization , because so far empirical evidence suggests that the performance difference between the various indexes that can service a scan is small. This is because scans over the different indexes have the same complexity, it is only locality effects that make one index more suitable than another. The difference between the best and worst plan may be a factor two in extreme cases, but not an order of magnitude. Although the optimizer performs both macro and micro-optimization, currently the indexes used can not be controlled by the query. Perhaps it would be best for the optimizer to only consider plans that the planner generates. Optimizing statement order For a query with n statements, there are n! possible statement orders. Trying every permutation quickly becomes prohibitive, especially because many permutations lead to terrible query plans, so even trying each plan once can take a long time. Fortunately, with some reasonable assumptions, it is possible to find a good plan quickly. Assumption : Adding an extra statement at the end of a query, can not make it faster. This assumption is justified, because every statement translates to a scan over an index, so adding a statement results in strictly more work. With the above assumption, we can express statement order optimization as a tree search problem. The root of the tree is an empty query, and along the branches we include one more statement of the original query. Interior nodes of the tree are partial queries , while the leaves form all permutations of the statements. To find the optimal query plan is to pick the best leaf node, and the assumption we made implies that the query time of a child node is larger than the query time of the parent node. Given the tree of partial queries, we can explore the tree to find the leaf with the minimal query time. Because the query time of a child is greater than that of the parent, we can call the additional time the \u201ccost\u201d of an edge, and finding the leaf with the minimal query time means finding a minimal-cost path from the root to a leaf. Dijkstra\u2019s algorithm solves this neatly: Track an open set of candidate nodes. Remove the candidate with the minimal query time from the open set, and add all of its children. If the best candidate in the open set is a leaf node, then that is the optimal query plan. While this algorithm will find the optimal query plan, it is not obvious that it will find it quickly. The algorithm might explore breadth-first, and explore most of the tree before it reaches a leaf node. But in practice, this is not what happens. Early on, there tend to be a few partial queries that are so bad that they are worse than many leaf nodes. This cuts off entire branches of the tree, and the algorithm reaches a leaf node quickly. Optimizing scans Some scans can be serviced by multiple indexes. For example, for a partial index scan that finds the value of an attribute for a given entity, we could use either EAVT or AEVT . For a given statement order, the goal of scan optimization is to find the best index to use for each statement. Unlike statement order optimization, there is no property that allows us to find a good solution incrementally. Because of locality interactions between scans, we need to consider the query plan as a whole. The current approach is to just try all possibilities. It is exponential, but not nearly as bad as permutations of statements. A statement can be serviced by 1, 2, or 3 possible indexes, so the number of options tends to be in the dozens, which is quite doable. This will surely blow up if you enter a ridiculously long query. Perhaps deleting the micro-optimizer is the best remedy. Explore-exploit This section has not been written yet. The gist of it is: Benchmarking is hard. Take the minimum. Justify min * (1 - 1/sqrt(n)) ordering. Probablistic, so not shortest path, but no problem, we only want a good path.","title":"Query optimization"},{"location":"query-optimization/#query-optimization","text":"The order of statements in a query affect the query plan that Noblit chooses. As outlined in the Query planning chapter, every statement translates to a loop that scans over an index. The planner preserves statement order: the first statement becomes the outer loop, the last statement becomes the inner loop. Therefore, the statement order of the query can have a big effect on query performance. To help optimize the order of statements, Noblit features a query optimizer. The optimizer is not part of the query planner. It is a standalone function that takes a query and a database, and returns the optimized query. There are two ways to optimize a query plan: Optimizing statement order , also called macro-optimization , because statement order can make orders of magnitude difference in the evaluation time of a query. This is because statement order can make a difference in complexity, it can be the difference between a query plan that is quadratic in the size of an index, versus a query plan that is constant time. Statement order can be the difference between microseconds, and minutes. Choosing the indexes to use for a scan , also called micro-optimization , because so far empirical evidence suggests that the performance difference between the various indexes that can service a scan is small. This is because scans over the different indexes have the same complexity, it is only locality effects that make one index more suitable than another. The difference between the best and worst plan may be a factor two in extreme cases, but not an order of magnitude. Although the optimizer performs both macro and micro-optimization, currently the indexes used can not be controlled by the query. Perhaps it would be best for the optimizer to only consider plans that the planner generates.","title":"Query optimization"},{"location":"query-optimization/#optimizing-statement-order","text":"For a query with n statements, there are n! possible statement orders. Trying every permutation quickly becomes prohibitive, especially because many permutations lead to terrible query plans, so even trying each plan once can take a long time. Fortunately, with some reasonable assumptions, it is possible to find a good plan quickly. Assumption : Adding an extra statement at the end of a query, can not make it faster. This assumption is justified, because every statement translates to a scan over an index, so adding a statement results in strictly more work. With the above assumption, we can express statement order optimization as a tree search problem. The root of the tree is an empty query, and along the branches we include one more statement of the original query. Interior nodes of the tree are partial queries , while the leaves form all permutations of the statements. To find the optimal query plan is to pick the best leaf node, and the assumption we made implies that the query time of a child node is larger than the query time of the parent node. Given the tree of partial queries, we can explore the tree to find the leaf with the minimal query time. Because the query time of a child is greater than that of the parent, we can call the additional time the \u201ccost\u201d of an edge, and finding the leaf with the minimal query time means finding a minimal-cost path from the root to a leaf. Dijkstra\u2019s algorithm solves this neatly: Track an open set of candidate nodes. Remove the candidate with the minimal query time from the open set, and add all of its children. If the best candidate in the open set is a leaf node, then that is the optimal query plan. While this algorithm will find the optimal query plan, it is not obvious that it will find it quickly. The algorithm might explore breadth-first, and explore most of the tree before it reaches a leaf node. But in practice, this is not what happens. Early on, there tend to be a few partial queries that are so bad that they are worse than many leaf nodes. This cuts off entire branches of the tree, and the algorithm reaches a leaf node quickly.","title":"Optimizing statement order"},{"location":"query-optimization/#optimizing-scans","text":"Some scans can be serviced by multiple indexes. For example, for a partial index scan that finds the value of an attribute for a given entity, we could use either EAVT or AEVT . For a given statement order, the goal of scan optimization is to find the best index to use for each statement. Unlike statement order optimization, there is no property that allows us to find a good solution incrementally. Because of locality interactions between scans, we need to consider the query plan as a whole. The current approach is to just try all possibilities. It is exponential, but not nearly as bad as permutations of statements. A statement can be serviced by 1, 2, or 3 possible indexes, so the number of options tends to be in the dozens, which is quite doable. This will surely blow up if you enter a ridiculously long query. Perhaps deleting the micro-optimizer is the best remedy.","title":"Optimizing scans"},{"location":"query-optimization/#explore-exploit","text":"This section has not been written yet. The gist of it is: Benchmarking is hard. Take the minimum. Justify min * (1 - 1/sqrt(n)) ordering. Probablistic, so not shortest path, but no problem, we only want a good path.","title":"Explore-exploit"},{"location":"query-planning/","text":"Query planning Noblit features a straightforward and transparent query planner. On the one hand, this means that query performance can vary a lot depending on how the query is structured. On the other hand, this gives you a lot of control over the query plan, and it ensures predictible performance. Plans Noblit evaluates every query as a set of nested loops that scan over its indexes. Every where-statement in the query translates to one loop. The query plan determines the order of those loops, and which index to scan for each loop. There are three kinds of scan in Noblit: A full index scan. Such a scan can be used for statements where both the entity and the value are unknown. A full index scan provides two variables in one loop, but it may have to scan the entire index. For scans over attribute-leading indexes, the range is still constrained by attribute. A partial index scan. Such a scan can be used for statements where either the entity or the value is known, either because it was provided by an outer loop, or because it is a constant in the query. A partial index scan can be used to find a single datom (for example, to look up the value of an attribute for a known entity in the EAVT index), or to look up a range of datoms (for example, to get all entities where an attribute has a known value in the AVET index). An existence test. Such a scan can be used to test if a datom exists in the database when both the entity and value are known. If it does not exists, the loop associated with this scan will have zero iterations (for a given iteration of the outer loops). The variables in a statement determine which scan is used: If both the entity and value are variable, and neither of these variables was referenced in an earlier statement, a full index scan will be used. If the statement contains one variable that was not referenced in an earlier statement, a partial index scan will be used. If all variables in the statement were referenced in earlier statements, an existence test will be used. Consequently, the order of statements in the query completely specifies what kind of scans are used, and which variables are provided by every loop. There is, however, some freedom left to the planner. When a scan can be serviced by multiple indexes, the planner decides which one to use. For example, to look up the user.name attribute of a known entity, we could use either EAVT , or AEVT . The former facilitates row access like a traditional relational database, whereas the latter facilitates column access like in a column database. It depends on the full query which one is preferred. If we are only selecting user.name , then column access would be preferred. But if we are also selecting a few dozen other attributes of the user, then row access would provide better locality. The planner uses heuristics to select the index to use for each scan. TODO: The planner should take schema into account. For example, for a single-nonunique attribute, we expect a partial EAVT scan to be cheaper than a partial AVET scan. But for a many-unique attribute, the converse is true. Ordering TODO: Write about interaction between ordering constraints and the query plan. Advice Warning: The advice here is just a guess, and not based on measurements. Given the way that evaluation and the query planner work, it makes sense to choose a statement order that constrains the ranges scanned as much as possible, to minimize the number of loop iterations. Query optimization Noblit does provide a query optimizer. Unlike traditional relational databases, optimization is an explicit operation, it is not implicitly part of every query. With the burden of runtime query plan optimization out of the way, Noblit can focus on an off-line optimizer. This has several advantages over runtime query optimization: Because the runtime query planner is very simple, it is very fast. There is less of a trade-off between spending time on planning and spending time on evaluation. Off-line (in the sense that it runs when instructed to, not implicitly as part of every query) we can afford to spend more time optimizing. Now that we can afford to spend more time, the optimizer can measure , it does not have to guess. Instead of using statistics to try and estimate the cost of a given plan, Noblit can profile the query against your actual database. Now that the optimizer measures, rather than estimates, there is no need to keep statistics about the data to base the estimates on. This eliminates a lot of complexity. This optimization scheme is made possible by the control that a straightforward query planner provides. If the planner would perform more advanced runtime optimization, then there would be less opportunity to tell it exactly what kind of plan we want. Of course this optimization scheme has downsides too. In particular, off-line optimization is not always possible or desirable. For ad-hoc one-off queries, you have to keep performance in mind. When representative data is unavailable (for example, when you only have a test database, not production data), results of off-line optimization may not be representative of real-world performance. Rules of thumb can help to ensure reasonable plans, but are no substitute for profiling. There is no way to optimize generated or partially generated queries. What you can do though, is optimize a few generated queries, and use the lessons learned to tweak the generator. The internals of the query optimizer are outlined in the Query optimization chapter.","title":"Query planning"},{"location":"query-planning/#query-planning","text":"Noblit features a straightforward and transparent query planner. On the one hand, this means that query performance can vary a lot depending on how the query is structured. On the other hand, this gives you a lot of control over the query plan, and it ensures predictible performance.","title":"Query planning"},{"location":"query-planning/#plans","text":"Noblit evaluates every query as a set of nested loops that scan over its indexes. Every where-statement in the query translates to one loop. The query plan determines the order of those loops, and which index to scan for each loop. There are three kinds of scan in Noblit: A full index scan. Such a scan can be used for statements where both the entity and the value are unknown. A full index scan provides two variables in one loop, but it may have to scan the entire index. For scans over attribute-leading indexes, the range is still constrained by attribute. A partial index scan. Such a scan can be used for statements where either the entity or the value is known, either because it was provided by an outer loop, or because it is a constant in the query. A partial index scan can be used to find a single datom (for example, to look up the value of an attribute for a known entity in the EAVT index), or to look up a range of datoms (for example, to get all entities where an attribute has a known value in the AVET index). An existence test. Such a scan can be used to test if a datom exists in the database when both the entity and value are known. If it does not exists, the loop associated with this scan will have zero iterations (for a given iteration of the outer loops). The variables in a statement determine which scan is used: If both the entity and value are variable, and neither of these variables was referenced in an earlier statement, a full index scan will be used. If the statement contains one variable that was not referenced in an earlier statement, a partial index scan will be used. If all variables in the statement were referenced in earlier statements, an existence test will be used. Consequently, the order of statements in the query completely specifies what kind of scans are used, and which variables are provided by every loop. There is, however, some freedom left to the planner. When a scan can be serviced by multiple indexes, the planner decides which one to use. For example, to look up the user.name attribute of a known entity, we could use either EAVT , or AEVT . The former facilitates row access like a traditional relational database, whereas the latter facilitates column access like in a column database. It depends on the full query which one is preferred. If we are only selecting user.name , then column access would be preferred. But if we are also selecting a few dozen other attributes of the user, then row access would provide better locality. The planner uses heuristics to select the index to use for each scan. TODO: The planner should take schema into account. For example, for a single-nonunique attribute, we expect a partial EAVT scan to be cheaper than a partial AVET scan. But for a many-unique attribute, the converse is true.","title":"Plans"},{"location":"query-planning/#ordering","text":"TODO: Write about interaction between ordering constraints and the query plan.","title":"Ordering"},{"location":"query-planning/#advice","text":"Warning: The advice here is just a guess, and not based on measurements. Given the way that evaluation and the query planner work, it makes sense to choose a statement order that constrains the ranges scanned as much as possible, to minimize the number of loop iterations.","title":"Advice"},{"location":"query-planning/#query-optimization","text":"Noblit does provide a query optimizer. Unlike traditional relational databases, optimization is an explicit operation, it is not implicitly part of every query. With the burden of runtime query plan optimization out of the way, Noblit can focus on an off-line optimizer. This has several advantages over runtime query optimization: Because the runtime query planner is very simple, it is very fast. There is less of a trade-off between spending time on planning and spending time on evaluation. Off-line (in the sense that it runs when instructed to, not implicitly as part of every query) we can afford to spend more time optimizing. Now that we can afford to spend more time, the optimizer can measure , it does not have to guess. Instead of using statistics to try and estimate the cost of a given plan, Noblit can profile the query against your actual database. Now that the optimizer measures, rather than estimates, there is no need to keep statistics about the data to base the estimates on. This eliminates a lot of complexity. This optimization scheme is made possible by the control that a straightforward query planner provides. If the planner would perform more advanced runtime optimization, then there would be less opportunity to tell it exactly what kind of plan we want. Of course this optimization scheme has downsides too. In particular, off-line optimization is not always possible or desirable. For ad-hoc one-off queries, you have to keep performance in mind. When representative data is unavailable (for example, when you only have a test database, not production data), results of off-line optimization may not be representative of real-world performance. Rules of thumb can help to ensure reasonable plans, but are no substitute for profiling. There is no way to optimize generated or partially generated queries. What you can do though, is optimize a few generated queries, and use the lessons learned to tweak the generator. The internals of the query optimizer are outlined in the Query optimization chapter.","title":"Query optimization"},{"location":"query/","text":"Query Queries in Noblit are declarative, based on logic programming, inspired by Datalog. Queries consist of a number of statements (logical claims, not steps in a procedure) that relate variables. The result of a query are the possible assignments of values to the variables. Structure A query has three parts: Where , a collection of statements that are true of the answers. Select , the variables whose values will be returned. Order by , to control ordering. (Not implemented yet.) The where-part of a query consists of (entity, attribute, value) tuples. The entity and value can be variables or constants. By convention, below we use single-character variable names to refer to entities, and full names to refer to other types of values (strings and integers). Example As an example, select all tracks by Muse from a music database, ordered by release date and then by track number: where a artist.name \"Muse\" b album.artist a b album.title album_title b album.release.year year b album.release.month month b album.release.day day t track.album b t track.number number t track.title track_title select number, track_title, album_title order by year, month, day, number Select all tracks titled \"One\", their artist, and release year: where t track.title \"One\" t track.album b b album.release.year year b album.artist a a artist.name artist select artist, year Semantics To answer a query, identify every variable in the query with the set of values it can assume. The answer to the query is the cartesian product of these sets, filtered such that for every tuple in the product, a datom exists in the database for each of the where-statements. For example, consider the following database of (entity, attribute, value) pairs: 1 person.name \"Henk\" 2 person.name \"Klaas\" 3 person.name \"Piet\" 1 person.age 32 2 person.age 54 3 person.age 32 Suppose we want to find all pairs of people with the same age. We would query: where p person.age a q person.age a p person.name p_name q person.name q_name select p_name, q_name This query has five variables. Taking the Cartesian product, we get: p p_name q q_name a 1 Henk 1 Henk 32 2 Henk 1 Henk 32 ... ... 3 Piet 3 Piet 54 Filtering first by the first and last two statements ( p person.age a , p person.name p_name , and q person.name q_name ), we are left with only the tuples where a is the age of person p , and where the names and the person ids match: p p_name q q_name a 1 Henk 1 Henk 32 2 Klaas 1 Henk 52 ... ... 3 Piet 3 Piet 32 Filtering by the remaining statement q person.age a , we find all tuples that satisfy the query: p p_name q q_name a 1 Henk 1 Henk 32 3 Piet 1 Henk 32 2 Klaas 2 Klaas 52 3 Henk 3 Piet 32 3 Piet 3 Piet 32 Finally, keeping only the selected columns yields the answer: p_name q_name Henk Henk Piet Henk Klaas Klaas Henk Piet Piet Piet Aggregations Note: This is an idea, it has not been implemented yet. For aggregations such as sum , count , and min and max , we put the aggregate in the select part of the query. For example, in an order database, we could select the total amount billed in 2020: where i invoice.year 2020 i invoice.amount_eur amount_eur select sum(amount_eur) When mixing aggregates and non-aggregates, aggregates are taken over the group keyed on the non-aggregate variables. For example, we may compute the total amount billed per year: where i invoice.year year i invoice.amount_eur amount_eur select year, sum(amount_eur) The above query will return one row per distinct year, it will not return the same year twice. This is in contrast to the non-aggregate query, which would return a row for every invoice entity. Aggregates can always be streamed, the grouping behavior does never cause a collection to be materialized in memory. This is because a query is evaluated as a nested loop. Non-aggregated variables are on the outer loops, and aggregated variables on the inner loops. This means that we visit one group key at a time.","title":"Query"},{"location":"query/#query","text":"Queries in Noblit are declarative, based on logic programming, inspired by Datalog. Queries consist of a number of statements (logical claims, not steps in a procedure) that relate variables. The result of a query are the possible assignments of values to the variables.","title":"Query"},{"location":"query/#structure","text":"A query has three parts: Where , a collection of statements that are true of the answers. Select , the variables whose values will be returned. Order by , to control ordering. (Not implemented yet.) The where-part of a query consists of (entity, attribute, value) tuples. The entity and value can be variables or constants. By convention, below we use single-character variable names to refer to entities, and full names to refer to other types of values (strings and integers).","title":"Structure"},{"location":"query/#example","text":"As an example, select all tracks by Muse from a music database, ordered by release date and then by track number: where a artist.name \"Muse\" b album.artist a b album.title album_title b album.release.year year b album.release.month month b album.release.day day t track.album b t track.number number t track.title track_title select number, track_title, album_title order by year, month, day, number Select all tracks titled \"One\", their artist, and release year: where t track.title \"One\" t track.album b b album.release.year year b album.artist a a artist.name artist select artist, year","title":"Example"},{"location":"query/#semantics","text":"To answer a query, identify every variable in the query with the set of values it can assume. The answer to the query is the cartesian product of these sets, filtered such that for every tuple in the product, a datom exists in the database for each of the where-statements. For example, consider the following database of (entity, attribute, value) pairs: 1 person.name \"Henk\" 2 person.name \"Klaas\" 3 person.name \"Piet\" 1 person.age 32 2 person.age 54 3 person.age 32 Suppose we want to find all pairs of people with the same age. We would query: where p person.age a q person.age a p person.name p_name q person.name q_name select p_name, q_name This query has five variables. Taking the Cartesian product, we get: p p_name q q_name a 1 Henk 1 Henk 32 2 Henk 1 Henk 32 ... ... 3 Piet 3 Piet 54 Filtering first by the first and last two statements ( p person.age a , p person.name p_name , and q person.name q_name ), we are left with only the tuples where a is the age of person p , and where the names and the person ids match: p p_name q q_name a 1 Henk 1 Henk 32 2 Klaas 1 Henk 52 ... ... 3 Piet 3 Piet 32 Filtering by the remaining statement q person.age a , we find all tuples that satisfy the query: p p_name q q_name a 1 Henk 1 Henk 32 3 Piet 1 Henk 32 2 Klaas 2 Klaas 52 3 Henk 3 Piet 32 3 Piet 3 Piet 32 Finally, keeping only the selected columns yields the answer: p_name q_name Henk Henk Piet Henk Klaas Klaas Henk Piet Piet Piet","title":"Semantics"},{"location":"query/#aggregations","text":"Note: This is an idea, it has not been implemented yet. For aggregations such as sum , count , and min and max , we put the aggregate in the select part of the query. For example, in an order database, we could select the total amount billed in 2020: where i invoice.year 2020 i invoice.amount_eur amount_eur select sum(amount_eur) When mixing aggregates and non-aggregates, aggregates are taken over the group keyed on the non-aggregate variables. For example, we may compute the total amount billed per year: where i invoice.year year i invoice.amount_eur amount_eur select year, sum(amount_eur) The above query will return one row per distinct year, it will not return the same year twice. This is in contrast to the non-aggregate query, which would return a row for every invoice entity. Aggregates can always be streamed, the grouping behavior does never cause a collection to be materialized in memory. This is because a query is evaluated as a nested loop. Non-aggregated variables are on the outer loops, and aggregated variables on the inner loops. This means that we visit one group key at a time.","title":"Aggregations"},{"location":"resources/","text":"Resources Below are some resources that were helpful or inspiring for the development of Noblit. Some of them were not directly useful, but simply interesting. Deconstructing the Database \u2014 a talk about Datomic and its data model The Datomic Data Model \u2014 from the Datomic documentation LevelDB implementation notes \u2014 from the LevelDB documentation InfluxDB Storage Engine Internals \u2014 a talk about storage in a time series database What You Always Wanted to Know About Datalog \u2014 a paper on Datalog Unofficial Guide to Datomic Internals \u2014 a blog post on Datomic internals SQLite: A Database for the Edge of the Network \u2014 a talk about SQLite internals The CMU database group channel \u2014 recorded lectures from their database courses Stardog Query Optimiser \u2014 about cardinality analysis in Stardog, a graph database","title":"Resources"},{"location":"resources/#resources","text":"Below are some resources that were helpful or inspiring for the development of Noblit. Some of them were not directly useful, but simply interesting. Deconstructing the Database \u2014 a talk about Datomic and its data model The Datomic Data Model \u2014 from the Datomic documentation LevelDB implementation notes \u2014 from the LevelDB documentation InfluxDB Storage Engine Internals \u2014 a talk about storage in a time series database What You Always Wanted to Know About Datalog \u2014 a paper on Datalog Unofficial Guide to Datomic Internals \u2014 a blog post on Datomic internals SQLite: A Database for the Edge of the Network \u2014 a talk about SQLite internals The CMU database group channel \u2014 recorded lectures from their database courses Stardog Query Optimiser \u2014 about cardinality analysis in Stardog, a graph database","title":"Resources"},{"location":"examples/bug-tracker/","text":"Bug Tracker Example Note: This example is outdated. For a working, albeit less documented example, take a look at the files in the \u2018golden\u2019 directory of the repository. Suppose we would want to build a database for a bug tracker, where users can file and comment on issues, and set and change certain properties. This is an excellent use case for an immutable database: we want to record status changes as new facts, but never lose historic data. Schema and Initial Provisioning We start by setting up attributes to define the schema, similar to how we would issue CREATE TABLE statements in a SQL database. Unlike in SQL, schema is reified in Noblit. This means that the schema is data, and it can be manipulated using the same query language as domain data. where $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" define attribute($a, $name, $type, $unique, $many): $a db.attribute.name $name $a db.attribute.type $type $a db.attribute.unique $unique $a db.attribute.many $many assert attribute($user_name, \"user.name\", $string_t, true, false) attribute($issue_title, \"issue.title\", $string_t, true, false) attribute($issue_description, \"issue.description\", $string_t, false, false) attribute($issue_priority, \"issue.priority\", $uint64_t, false, false) attribute($issue_author, \"issue.author\", $ref_t, false, false) attribute($issue_comment, \"issue.comment\", $ref_t, true, false) attribute($comment_content, \"comment.content\", $string_t, false, false) attribute($comment_author, \"comment.author\", $ref_t, false, false) where -- Lookp up the data types by name, so we can refer to these types when -- defining new attributes. $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" assert -- Note: these definitions are verbose. Noblit's query language does not -- offer abstractions such as functions to reduce verbosity. Rather, you -- can build abstractions on top of a client library. Because Noblit has -- a simple data model, and because Noblit accepts data structures rather -- than strings as queries, it is easy and safe to build such abstractions -- in your programming language, rather than into the query language. $author_name db.attribute.name \"author.name\" $author_name db.attribute.type $string_t $author_name db.attribute.unique true $author_name db.attribute.many false $issue_title db.attribute.name \"issue.title\" $issue_title db.attribute.type $string_t $issue_title db.attribute.unique true $issue_title db.attribute.many false $issue_description db.attribute.name \"issue.description\" $issue_description db.attribute.type $string_t $issue_description db.attribute.unique false $issue_description db.attribute.many false $issue_priority db.attribute.name \"issue.priority\" $issue_priority db.attribute.type $uint64_t $issue_priority db.attribute.unique false $issue_priority db.attribute.many false $issue_author db.attribute.name \"issue.author\" $issue_author db.attribute.type $ref_t $issue_author db.attribute.unique false $issue_author db.attribute.many false -- Experimental idea: attribute constraints (like foreign keys). -- The author ref must point to an entity that has the \"author.name\" -- attribute. $issue_author db.constraint.has_attribute $author_name $comment_content db.attribute.name \"comment.content\" $comment_content db.attribute.type $string_t $comment_content db.attribute.unique false $comment_content db.attribute.many false $comment_author db.attribute.name \"comment.author\" $comment_author db.attribute.type $ref_t $comment_author db.attribute.unique false $comment_author db.attribute.many false $comment_author db.constraint.has_attribute author_name -- TODO: Add created date_time_offset to issue and comment. -- Finally, an issue can have multiple comments associated with it. We -- could model this relation in two ways: by giving the comment a -- \"comment.issue\" attribute, or by giving the issue an \"issue.comment\" -- attribute. For the sake of demonstration we will go with the latter. -- Note that because Noblit maintains an (attribute, entity, value) index -- as well as an (attribute, value, entity) index, finding all comments -- associated with an issue is efficient in any case. $issue_comment db.attribute.name \"issue.comment\" $issue_comment db.attribute.type $ref_t $issue_comment db.attribute.unique false -- We set \"many\" to true: an issue can have many comments; the attribute -- may be present zero or more times. $issue_comment db.attribute.many true $issue_comment db.constraint.has_attribute $author_name select $issue_title, $issue_description, $issue_priority, $issue_author This will set up thee kinds of entities: authors, issues, and comments. These kinds are only encoded in the attribute names, there are no tables like in a traditional database. The attribute ids of issue.title , issue.description , issue.priority , and issue.author will be returned. Let's insert some initial data: assert $batty author.name \"Roy Batty\" $i issue.title \"Lifespan is too short\" $i issue.description \"Can the maker repair what he makes? I want more life.\" $i issue.priority 0 $i issue.author $batty select $i Now we have one issue. The query returns the entity id of the issue. Let's close the issue as wontfix. But wait a mintue ... we forgot to add a status attribute! Let's define one right now. where $string_t db.type.name \"string\" $ref_t db.type.name \"ref\" assert -- Define an enum for statuses, by defining a single \"status.name\" -- attribute that must have a unique value among all instances of this -- attribute. $status_name db.attribute.name \"status.name\" $status_name db.attribute.type $string_t $status_name db.attribute.unique true $status_name db.attribute.many false -- Then we define the issue statuses. We reference the new attribute by -- variable here. It could not be referenced by name, because at the time -- the query is constructed, it does not yet exist. $open_unconfirmed $status_name \"open/unconfirmed\" $open_confirmed $status_name \"open/confirmed\" $open_in_progress $status_name \"open/in-progress\" $closed_fixed $status_name \"closed/fixed\" $closed_obsolete $status_name \"closed/obsolete\" $closed_wontfix $status_name \"closed/wont-fix\" -- Finally, an issue can have a status. $issue_status db.attribute.name \"issue.status\" $issue_status db.attribute.type $ref_t $issue_status db.attribute.unique false $issue_status db.attribute.many false $issue_comment db.constraint.has_attribute $status_name Note that at this point, although she issue.status attribute exists, our first issue does not have the attribute. TODO: How to deal with implied attributes? (Sort of like non-nullable, except that there is no null in Noblit, only the presence or absence of an attribute.) Add a constraint that issue.title implies issue.status (and the others)? And then we'd be forced to add a status to all issues that don't have one in the same transaction that adds the attribute. In any case, let's set the issue to open/confirmed. We could reference the issue and status by entity id directly, but for demonstration purposes, we will look it up by title instead. We marked the issue.title and status.name attributes are unique, so this will affect at most one entity. where $issue issue.title \"Lifespan is too short\" $open status.name \"open/confirmed\" assert $issue issue.status $open Now we can close the issue. Note that because the issue.status attribute has its cardinality attribute.many set to false, we can't just assert a new status. We also have to retract the old status. The change is atomic, in one transaction. where $i issue.title \"Lifespan is too short\" $i issue.status $old_status $wontfix status.name \"closed/wont-fix\" retract $i issue.status $old_status assert $tyrell author.name \"Eldon Tyrell\" $c comment.content \"The light that burns twice as bright burns half as long.\" $c comment.author $tyrell $i issue.comment $c $i issue.status $wontfix A Selection of Selects Suppose we wanted to list all issues, ordered by priority: where $i issue.title $title $i issue.priority $p select $i, $p, $title order by $p asc This will return triples of issue entity id, priority, and title, ordered by priority (supposing we took lower integers to indicate more urgent issues). TODO: How to filter on only open issues? Would need an in or or clause. Suppose we wanted to list all issues created by Roy Batty: where $batty author.name \"Roy Batty\" $i issue.author $batty $i issue.title $title select $i, $title Now we get the entity ids of the issues created by Roy, as well as their titles. Suppose we want to find all issues where Rick Deckard left a comment: where $deckard author.name \"Rick Deckard\" $c comment.author $deckard $i issue.comment $c $i issue.title $title select $i, $title We again get all issues and titles. We will get a cartesian product of issues and comments by Deckard though: if Deckard commented twice, the issue will be in the result set twice. TODO: How to address that? select unique ? Or more fine-grained control? Or just order by the result and uniq it in the client library? Querying History After our issue was created, the status changed from open/unfonfirmed to closed/wont-fix . However, if we query just the status, we can only see the current status. The query where $i issue.title \"Lifespan is too short\" $i issue.status $status select $status returns closed/wont-fix . If we wanted to construct a timeline of the status changes that the issue underwent, we could include historic datoms like so: where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert select $t, $prev_status, $new_status order by $t asc In a regular where query, only datoms that have not been retracted will match. A where historic query works differently: it queries all datoms, and it matches both assertions and retractons alike. To filter these, a historical query takes 5-tuples rather than 3-tuples. In addition to (entity, attribute, value), the query takes a transaction id and operation (assert or retract). In this case, we select prev_status and new_status , such that the status prev_status was retracted on issue i in transaction t , and new_status was asserted in the same transaction. This gets us a list of status changes, ordered by transaction id. Note that the query to locate the issue by title is not in a where historic clause. If it were, we would get the status changes of all issues that had been called Lifespan too short a t some point, not only the history of the issue that currently has that title. Transactions in Noblit are reified. They are entities that can have attributes. Suppose we had added a transaction.date_time_offset attribute to every transaction, and also a transaction.initiator . Then we could also find out when the attribute changed, and who made that change. where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert where $t transaction.date_time_offset $time $t transaction.initiator $initiator $initiator author.name $initiator_name select $time, $initiator_name, $prev_status, $new_status order by $time asc, $t asc Now we get a nice chronological list of status changes, along with the name of the user who made the change. TODO: Rename author.name to user.name ? TODO: Add section define transaction attributes.","title":"Bug tracker"},{"location":"examples/bug-tracker/#bug-tracker-example","text":"Note: This example is outdated. For a working, albeit less documented example, take a look at the files in the \u2018golden\u2019 directory of the repository. Suppose we would want to build a database for a bug tracker, where users can file and comment on issues, and set and change certain properties. This is an excellent use case for an immutable database: we want to record status changes as new facts, but never lose historic data.","title":"Bug Tracker Example"},{"location":"examples/bug-tracker/#schema-and-initial-provisioning","text":"We start by setting up attributes to define the schema, similar to how we would issue CREATE TABLE statements in a SQL database. Unlike in SQL, schema is reified in Noblit. This means that the schema is data, and it can be manipulated using the same query language as domain data. where $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" define attribute($a, $name, $type, $unique, $many): $a db.attribute.name $name $a db.attribute.type $type $a db.attribute.unique $unique $a db.attribute.many $many assert attribute($user_name, \"user.name\", $string_t, true, false) attribute($issue_title, \"issue.title\", $string_t, true, false) attribute($issue_description, \"issue.description\", $string_t, false, false) attribute($issue_priority, \"issue.priority\", $uint64_t, false, false) attribute($issue_author, \"issue.author\", $ref_t, false, false) attribute($issue_comment, \"issue.comment\", $ref_t, true, false) attribute($comment_content, \"comment.content\", $string_t, false, false) attribute($comment_author, \"comment.author\", $ref_t, false, false) where -- Lookp up the data types by name, so we can refer to these types when -- defining new attributes. $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" assert -- Note: these definitions are verbose. Noblit's query language does not -- offer abstractions such as functions to reduce verbosity. Rather, you -- can build abstractions on top of a client library. Because Noblit has -- a simple data model, and because Noblit accepts data structures rather -- than strings as queries, it is easy and safe to build such abstractions -- in your programming language, rather than into the query language. $author_name db.attribute.name \"author.name\" $author_name db.attribute.type $string_t $author_name db.attribute.unique true $author_name db.attribute.many false $issue_title db.attribute.name \"issue.title\" $issue_title db.attribute.type $string_t $issue_title db.attribute.unique true $issue_title db.attribute.many false $issue_description db.attribute.name \"issue.description\" $issue_description db.attribute.type $string_t $issue_description db.attribute.unique false $issue_description db.attribute.many false $issue_priority db.attribute.name \"issue.priority\" $issue_priority db.attribute.type $uint64_t $issue_priority db.attribute.unique false $issue_priority db.attribute.many false $issue_author db.attribute.name \"issue.author\" $issue_author db.attribute.type $ref_t $issue_author db.attribute.unique false $issue_author db.attribute.many false -- Experimental idea: attribute constraints (like foreign keys). -- The author ref must point to an entity that has the \"author.name\" -- attribute. $issue_author db.constraint.has_attribute $author_name $comment_content db.attribute.name \"comment.content\" $comment_content db.attribute.type $string_t $comment_content db.attribute.unique false $comment_content db.attribute.many false $comment_author db.attribute.name \"comment.author\" $comment_author db.attribute.type $ref_t $comment_author db.attribute.unique false $comment_author db.attribute.many false $comment_author db.constraint.has_attribute author_name -- TODO: Add created date_time_offset to issue and comment. -- Finally, an issue can have multiple comments associated with it. We -- could model this relation in two ways: by giving the comment a -- \"comment.issue\" attribute, or by giving the issue an \"issue.comment\" -- attribute. For the sake of demonstration we will go with the latter. -- Note that because Noblit maintains an (attribute, entity, value) index -- as well as an (attribute, value, entity) index, finding all comments -- associated with an issue is efficient in any case. $issue_comment db.attribute.name \"issue.comment\" $issue_comment db.attribute.type $ref_t $issue_comment db.attribute.unique false -- We set \"many\" to true: an issue can have many comments; the attribute -- may be present zero or more times. $issue_comment db.attribute.many true $issue_comment db.constraint.has_attribute $author_name select $issue_title, $issue_description, $issue_priority, $issue_author This will set up thee kinds of entities: authors, issues, and comments. These kinds are only encoded in the attribute names, there are no tables like in a traditional database. The attribute ids of issue.title , issue.description , issue.priority , and issue.author will be returned. Let's insert some initial data: assert $batty author.name \"Roy Batty\" $i issue.title \"Lifespan is too short\" $i issue.description \"Can the maker repair what he makes? I want more life.\" $i issue.priority 0 $i issue.author $batty select $i Now we have one issue. The query returns the entity id of the issue. Let's close the issue as wontfix. But wait a mintue ... we forgot to add a status attribute! Let's define one right now. where $string_t db.type.name \"string\" $ref_t db.type.name \"ref\" assert -- Define an enum for statuses, by defining a single \"status.name\" -- attribute that must have a unique value among all instances of this -- attribute. $status_name db.attribute.name \"status.name\" $status_name db.attribute.type $string_t $status_name db.attribute.unique true $status_name db.attribute.many false -- Then we define the issue statuses. We reference the new attribute by -- variable here. It could not be referenced by name, because at the time -- the query is constructed, it does not yet exist. $open_unconfirmed $status_name \"open/unconfirmed\" $open_confirmed $status_name \"open/confirmed\" $open_in_progress $status_name \"open/in-progress\" $closed_fixed $status_name \"closed/fixed\" $closed_obsolete $status_name \"closed/obsolete\" $closed_wontfix $status_name \"closed/wont-fix\" -- Finally, an issue can have a status. $issue_status db.attribute.name \"issue.status\" $issue_status db.attribute.type $ref_t $issue_status db.attribute.unique false $issue_status db.attribute.many false $issue_comment db.constraint.has_attribute $status_name Note that at this point, although she issue.status attribute exists, our first issue does not have the attribute. TODO: How to deal with implied attributes? (Sort of like non-nullable, except that there is no null in Noblit, only the presence or absence of an attribute.) Add a constraint that issue.title implies issue.status (and the others)? And then we'd be forced to add a status to all issues that don't have one in the same transaction that adds the attribute. In any case, let's set the issue to open/confirmed. We could reference the issue and status by entity id directly, but for demonstration purposes, we will look it up by title instead. We marked the issue.title and status.name attributes are unique, so this will affect at most one entity. where $issue issue.title \"Lifespan is too short\" $open status.name \"open/confirmed\" assert $issue issue.status $open Now we can close the issue. Note that because the issue.status attribute has its cardinality attribute.many set to false, we can't just assert a new status. We also have to retract the old status. The change is atomic, in one transaction. where $i issue.title \"Lifespan is too short\" $i issue.status $old_status $wontfix status.name \"closed/wont-fix\" retract $i issue.status $old_status assert $tyrell author.name \"Eldon Tyrell\" $c comment.content \"The light that burns twice as bright burns half as long.\" $c comment.author $tyrell $i issue.comment $c $i issue.status $wontfix","title":"Schema and Initial Provisioning"},{"location":"examples/bug-tracker/#a-selection-of-selects","text":"Suppose we wanted to list all issues, ordered by priority: where $i issue.title $title $i issue.priority $p select $i, $p, $title order by $p asc This will return triples of issue entity id, priority, and title, ordered by priority (supposing we took lower integers to indicate more urgent issues). TODO: How to filter on only open issues? Would need an in or or clause. Suppose we wanted to list all issues created by Roy Batty: where $batty author.name \"Roy Batty\" $i issue.author $batty $i issue.title $title select $i, $title Now we get the entity ids of the issues created by Roy, as well as their titles. Suppose we want to find all issues where Rick Deckard left a comment: where $deckard author.name \"Rick Deckard\" $c comment.author $deckard $i issue.comment $c $i issue.title $title select $i, $title We again get all issues and titles. We will get a cartesian product of issues and comments by Deckard though: if Deckard commented twice, the issue will be in the result set twice. TODO: How to address that? select unique ? Or more fine-grained control? Or just order by the result and uniq it in the client library?","title":"A Selection of Selects"},{"location":"examples/bug-tracker/#querying-history","text":"After our issue was created, the status changed from open/unfonfirmed to closed/wont-fix . However, if we query just the status, we can only see the current status. The query where $i issue.title \"Lifespan is too short\" $i issue.status $status select $status returns closed/wont-fix . If we wanted to construct a timeline of the status changes that the issue underwent, we could include historic datoms like so: where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert select $t, $prev_status, $new_status order by $t asc In a regular where query, only datoms that have not been retracted will match. A where historic query works differently: it queries all datoms, and it matches both assertions and retractons alike. To filter these, a historical query takes 5-tuples rather than 3-tuples. In addition to (entity, attribute, value), the query takes a transaction id and operation (assert or retract). In this case, we select prev_status and new_status , such that the status prev_status was retracted on issue i in transaction t , and new_status was asserted in the same transaction. This gets us a list of status changes, ordered by transaction id. Note that the query to locate the issue by title is not in a where historic clause. If it were, we would get the status changes of all issues that had been called Lifespan too short a t some point, not only the history of the issue that currently has that title. Transactions in Noblit are reified. They are entities that can have attributes. Suppose we had added a transaction.date_time_offset attribute to every transaction, and also a transaction.initiator . Then we could also find out when the attribute changed, and who made that change. where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert where $t transaction.date_time_offset $time $t transaction.initiator $initiator $initiator author.name $initiator_name select $time, $initiator_name, $prev_status, $new_status order by $time asc, $t asc Now we get a nice chronological list of status changes, along with the name of the user who made the change. TODO: Rename author.name to user.name ? TODO: Add section define transaction attributes.","title":"Querying History"},{"location":"examples/key-value-lookup/","text":"Key-value lookup example This example shows how to use Noblit as a key-value store. In particular, we will be inserting password hashes and counts from a Have I Been Pwned dump into a database, so we can check whether a given password occurred in a breach. A dump includes not only the SHA1 hashes of breached passwords, but also their prevalence count. This is an excellent use case for a key-value store: the SHA1 hashes will be the keys, and the prevalence counts will be the values. A key-value store is a simple example, and not one that Noblit excels at. We will import the Have I been Pwned dump into a new database in a single run, and only read from the database after that. As such, this example does not showcase Noblit\u2019s history-related features. Also, because Noblit is a general-purpose database, it is not going to be as performant or convenient as a specialized key-value store. Still, a key-value store is a good example to get started with, before moving on to more complex schemas or queries. The code You can find the finished example in the haveibeenpwned subdirectory of the examples directory in the root of the repository. Setting up the schema TODO: Write this section. We have pw.sha1 of type bytes . We have pw.count of type uint64 . Inserting TODO: Write this section. Insert in batches, transactions must fit in memory. Some fluff to read the dump file. Lookup TODO: Write this section. where pw pw.sha1 :sha1 pw pw.count count select count","title":"Key-value lookup"},{"location":"examples/key-value-lookup/#key-value-lookup-example","text":"This example shows how to use Noblit as a key-value store. In particular, we will be inserting password hashes and counts from a Have I Been Pwned dump into a database, so we can check whether a given password occurred in a breach. A dump includes not only the SHA1 hashes of breached passwords, but also their prevalence count. This is an excellent use case for a key-value store: the SHA1 hashes will be the keys, and the prevalence counts will be the values. A key-value store is a simple example, and not one that Noblit excels at. We will import the Have I been Pwned dump into a new database in a single run, and only read from the database after that. As such, this example does not showcase Noblit\u2019s history-related features. Also, because Noblit is a general-purpose database, it is not going to be as performant or convenient as a specialized key-value store. Still, a key-value store is a good example to get started with, before moving on to more complex schemas or queries.","title":"Key-value lookup example"},{"location":"examples/key-value-lookup/#the-code","text":"You can find the finished example in the haveibeenpwned subdirectory of the examples directory in the root of the repository.","title":"The code"},{"location":"examples/key-value-lookup/#setting-up-the-schema","text":"TODO: Write this section. We have pw.sha1 of type bytes . We have pw.count of type uint64 .","title":"Setting up the schema"},{"location":"examples/key-value-lookup/#inserting","text":"TODO: Write this section. Insert in batches, transactions must fit in memory. Some fluff to read the dump file.","title":"Inserting"},{"location":"examples/key-value-lookup/#lookup","text":"TODO: Write this section. where pw pw.sha1 :sha1 pw pw.count count select count","title":"Lookup"},{"location":"reference/c/","text":"C API reference Noblit exposes a C interface for use from non-Rust code. The C interface is an unsafe layer on top of the Rust crate that implements Noblit. The C interface is also written in Rust, but exposes unmangled symbols with C-compatible types. The C interface is intended to be used by client libraries and therefore minimal. There are no functions to construct or manipulate queries. Instead, Noblit accepts queries in a binary format, and expects client libraries to build and serialize queries. This ensures that only simple data types have to be passed across FFI boundaries, and it minimizes ownership transfer across FFI boundaries. This eliminates room for error and makes it easier to fuzz the remaining API . The Python and Haskell client libraries are built upon the C interface. noblit_t typedef struct noblit noblit_t; An opaque database handle. Internally, this structure contains a noblit::Database , with type parameters fixed to one of three cases: In-memory . In-memory databases are always mutable. Immutable on-disk . The files can be opened in read-only mode. (Not yet implemented.) Mutable on-disk . The files need to be opened writeable. (Not yet implemented.) Furthermore, the noblit_t structure stores the most recent error that occurred, if any. In Rust, errors are tracked through the Result type, and these are returned to the C interface as a status code, with separate getters for the error details in case of an error. TODO: Implement this. noblit_result_t typedef uint32_t noblit_result_t; Noblit functions return status codes. A zero return value means success, other values indicate failure. The status codes map to noblit::error::Error in Rust. If a function returns a non-zero status code, noblit_get_last_error will return a string that contains the full error message. #define NOBLIT_OK 0 #define NOBLIT_IO_ERROR 1 noblit_slice_t typedef struct noblit_slice { uint8_t const* data; size_t len; } noblit_slice_t; A view into an immutable byte array owned by Noblit. noblit_get_last_error noblit_slice_t noblit_get_last_error(noblit_t const* db); If the last call to a function that returns noblit_result_t returned a nonzero status code, this function returns the full error message string as UTF-8 bytes, excluding null terminator. The slice pointed to is valid until the next call to a Noblit function. If the message is needed for after that, the caller needs to memcpy it elsewhere. If a C-style null terminated string is desired, the caller needs to copy it into a buffer one longer than the slice, to accomodate the null terminator. noblit_open_packed_in_memory noblit_t* noblit_open_packed_in_memory(int fd); Load a database from a file in packed format into memory. The resulting database is mutable, but mutations are applied to the in-memory database. The file that it was loaded from is left untouched when the database is mutated. fd File descriptor of the file to read from. The file descriptor is borrowed mutably for the duration of the call. return value A pointer to a database struct. The pointer should be treated as an opaque pointer. noblit_open_packed_mmap noblit_t* noblit_open_packed_mmap(int fd); Map a database from a file in packed format into memory. The resulting database is immutable. fd File descriptor of the file to read from. The file descriptor is borrowed mutably for the duration of the call. return value A pointer to a database struct. The pointer should be treated as an opaque pointer. Memory-mapping a database has a severe caveat : there is no way to handle IO errors gracefully. The process will receive SIGBUS on IO errors. noblit_close void noblit_close(noblit_t* db); Close the database, release associated resources, and deallocate the noblit_t struct. noblit_evaluator_t typedef struct noblit_query noblit_evaluator_t; A query evaluator. noblit_query_open noblit_result_t noblit_query_open( noblit_t *const db, uint8_t *const query, size_t query_len, noblit_evaluator_t** evaluator ); Parse and plan a query, produce an evaluator to iterate the results. db Database to query. The database remains borrowed for as long as the evaluator exists; it should not be referenced until the matching noblit_query_close . query Buffer that contains the query in binary format. The query buffer is borrowed for the duration of the call, Noblit does not reference it after the call returns. See also noblit::binary module for the binary format reference. TODO: Document it properly. query_len Length of the query buffer in bytes. evaluator Out parameter for the resulting evaluator. When the function returns 0, *evaluator will contain a pointer to the evaluator. noblit_query_close void noblit_query_close(noblit_evaluator_t* evaluator); Take ownership of the evaluator and deallocate it.","title":"C API reference"},{"location":"reference/c/#c-api-reference","text":"Noblit exposes a C interface for use from non-Rust code. The C interface is an unsafe layer on top of the Rust crate that implements Noblit. The C interface is also written in Rust, but exposes unmangled symbols with C-compatible types. The C interface is intended to be used by client libraries and therefore minimal. There are no functions to construct or manipulate queries. Instead, Noblit accepts queries in a binary format, and expects client libraries to build and serialize queries. This ensures that only simple data types have to be passed across FFI boundaries, and it minimizes ownership transfer across FFI boundaries. This eliminates room for error and makes it easier to fuzz the remaining API . The Python and Haskell client libraries are built upon the C interface.","title":"C API reference"},{"location":"reference/c/#noblit_t","text":"typedef struct noblit noblit_t; An opaque database handle. Internally, this structure contains a noblit::Database , with type parameters fixed to one of three cases: In-memory . In-memory databases are always mutable. Immutable on-disk . The files can be opened in read-only mode. (Not yet implemented.) Mutable on-disk . The files need to be opened writeable. (Not yet implemented.) Furthermore, the noblit_t structure stores the most recent error that occurred, if any. In Rust, errors are tracked through the Result type, and these are returned to the C interface as a status code, with separate getters for the error details in case of an error. TODO: Implement this.","title":"noblit_t"},{"location":"reference/c/#noblit_result_t","text":"typedef uint32_t noblit_result_t; Noblit functions return status codes. A zero return value means success, other values indicate failure. The status codes map to noblit::error::Error in Rust. If a function returns a non-zero status code, noblit_get_last_error will return a string that contains the full error message. #define NOBLIT_OK 0 #define NOBLIT_IO_ERROR 1","title":"noblit_result_t"},{"location":"reference/c/#noblit_slice_t","text":"typedef struct noblit_slice { uint8_t const* data; size_t len; } noblit_slice_t; A view into an immutable byte array owned by Noblit.","title":"noblit_slice_t"},{"location":"reference/c/#noblit_get_last_error","text":"noblit_slice_t noblit_get_last_error(noblit_t const* db); If the last call to a function that returns noblit_result_t returned a nonzero status code, this function returns the full error message string as UTF-8 bytes, excluding null terminator. The slice pointed to is valid until the next call to a Noblit function. If the message is needed for after that, the caller needs to memcpy it elsewhere. If a C-style null terminated string is desired, the caller needs to copy it into a buffer one longer than the slice, to accomodate the null terminator.","title":"noblit_get_last_error"},{"location":"reference/c/#noblit_open_packed_in_memory","text":"noblit_t* noblit_open_packed_in_memory(int fd); Load a database from a file in packed format into memory. The resulting database is mutable, but mutations are applied to the in-memory database. The file that it was loaded from is left untouched when the database is mutated. fd File descriptor of the file to read from. The file descriptor is borrowed mutably for the duration of the call. return value A pointer to a database struct. The pointer should be treated as an opaque pointer.","title":"noblit_open_packed_in_memory"},{"location":"reference/c/#noblit_open_packed_mmap","text":"noblit_t* noblit_open_packed_mmap(int fd); Map a database from a file in packed format into memory. The resulting database is immutable. fd File descriptor of the file to read from. The file descriptor is borrowed mutably for the duration of the call. return value A pointer to a database struct. The pointer should be treated as an opaque pointer. Memory-mapping a database has a severe caveat : there is no way to handle IO errors gracefully. The process will receive SIGBUS on IO errors.","title":"noblit_open_packed_mmap"},{"location":"reference/c/#noblit_close","text":"void noblit_close(noblit_t* db); Close the database, release associated resources, and deallocate the noblit_t struct.","title":"noblit_close"},{"location":"reference/c/#noblit_evaluator_t","text":"typedef struct noblit_query noblit_evaluator_t; A query evaluator.","title":"noblit_evaluator_t"},{"location":"reference/c/#noblit_query_open","text":"noblit_result_t noblit_query_open( noblit_t *const db, uint8_t *const query, size_t query_len, noblit_evaluator_t** evaluator ); Parse and plan a query, produce an evaluator to iterate the results. db Database to query. The database remains borrowed for as long as the evaluator exists; it should not be referenced until the matching noblit_query_close . query Buffer that contains the query in binary format. The query buffer is borrowed for the duration of the call, Noblit does not reference it after the call returns. See also noblit::binary module for the binary format reference. TODO: Document it properly. query_len Length of the query buffer in bytes. evaluator Out parameter for the resulting evaluator. When the function returns 0, *evaluator will contain a pointer to the evaluator.","title":"noblit_query_open"},{"location":"reference/c/#noblit_query_close","text":"void noblit_query_close(noblit_evaluator_t* evaluator); Take ownership of the evaluator and deallocate it.","title":"noblit_query_close"},{"location":"reference/rust/","text":"Rust API reference The Rust API reference can be generated from the source code by running cargo doc in the repository root. Currently it is not hosted anywhere.","title":"Rust API reference"},{"location":"reference/rust/#rust-api-reference","text":"The Rust API reference can be generated from the source code by running cargo doc in the repository root. Currently it is not hosted anywhere.","title":"Rust API reference"},{"location":"theme/","text":"Kilsbergen A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more. Demos Musium documentation Noblit documentation Pris documentation Squiller documentation Tako documentation Features Responsive design Zero javascript Usage One easy way to use this theme, is to add it as a Git submodule to your docs directory, e.g. at docs/theme . Then add the following in your mkdocs.yml : theme: name: null custom_dir: docs/theme This theme requires MkDocs 1.1 or later. For earlier versions, delete this README.md to work around this bug . To enable anchors next to section headings, add the following to your mkdocs.yml : markdown_extensions: - toc: permalink: true permalink_title: null To enable syntax highlighting, ensure that pygmentize is available, and add the following to your mkdocs.yml : markdown_extensions: - codehilite See also the python-markdown list of extensions . License Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"Kilsbergen"},{"location":"theme/#kilsbergen","text":"A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more.","title":"Kilsbergen"},{"location":"theme/#demos","text":"Musium documentation Noblit documentation Pris documentation Squiller documentation Tako documentation","title":"Demos"},{"location":"theme/#features","text":"Responsive design Zero javascript","title":"Features"},{"location":"theme/#usage","text":"One easy way to use this theme, is to add it as a Git submodule to your docs directory, e.g. at docs/theme . Then add the following in your mkdocs.yml : theme: name: null custom_dir: docs/theme This theme requires MkDocs 1.1 or later. For earlier versions, delete this README.md to work around this bug . To enable anchors next to section headings, add the following to your mkdocs.yml : markdown_extensions: - toc: permalink: true permalink_title: null To enable syntax highlighting, ensure that pygmentize is available, and add the following to your mkdocs.yml : markdown_extensions: - codehilite See also the python-markdown list of extensions .","title":"Usage"},{"location":"theme/#license","text":"Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"License"}]}
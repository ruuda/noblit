{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Noblit Vaporware warning: much of the content below is hypothetical. Noblit is an embeddable append-only database. The database records a history of immutable (entity, attribute, value) tuples. Tuples can be asserted and retracted. A retraction is recorded as a new fact; it is not a delete. Any historical state of the database can be reproduced, and the history is first-class and queryable. Features Embeddable. Noblit comes as a library that you link into your application. Global installation or daemons are not necessary. Simple but flexible data model. Noblit stores (entity, attribute, value) tuples. A traditional relational model can be expressed like this, but less rigid structures such as graphs can be expressed too. Machine-friendly relational queries. Queries are data structures rather than strings. There is no need for string formatting or escaping, and the full power of the host language is available to safely generate and compose queries. Point in time queries. Any historical state of the database can be reproduced and queried efficiently. Reified schema . Noblit stores the schema as entities in the database itself. The schema can be evolved through normal assertions and retractions, and the full migration history is available. Reified transactions . Transactions in Noblit are entities that can have attributes like any other entity. Possibilities include the transaction timestamp and the user who initiated the transaction. Transactions can be inspected in queries. Profile-guided query optimization . While queries are declarative, statement order affects the query plan. An explicit query optimizer can optimize a given query by measuring how alternatives perform, instead of having to estimate from statistics. Comparison Noblit is heavily inspired by Datomic , especially by its data model. In comparison to other databases such as SQLite and Postgres , Noblit positions itself as follows: Client-server Embedded Mutable Postgres SQLite Immutable Datomic Noblit Noblit combines the low operational overhead of an embedded database with the simplicity of an append-only database with value semantics. In comparison to Datomic, Noblit puts more focus on static type safety. This makes schemas more rigid. On the one hand this impedes iterating quickly, but on the other hand it helps to provide data consistency in the long term. Datomic is a distributed system running on the JVM , while Noblit is a small native library. Another attempt at an embedded database inspired by Datomic was Mentat . Both Noblit and Mentat happen to be written in Rust. Mentat is a layer on top of SQLite, whereas Noblit has its own storage backend. Mentat exists but is unmaintained, Noblit is vaporware. Goals An embeddable library for queries and transactions. Enabling \u2014 but not requiring \u2014 a client-server database on top of the library, with a single transactor and scale-out reads. Storing datasets that do not fit in working memory. The result of a query and data to insert must fit though. Handling moderate write workloads and moderately sized data. Relational queries in a machine-first query format. Simplicity. First make it work, then make it fast. I am writing this down to remind myself to not micro-optimize. Complications should be justified by profiling measurements. Easy to build, few or zero dependencies. Non-goals Deleting data or updating data in place. Writing large values (say, larger than 64 KiB). A key-value store or document store might be better suited for this. High write throughput. If you produce large volumes of data quickly, probably not all of it remains valuable for a long time, and storage space might become a concern. A time series database or durable message queue might be a better alternative. Having the fastest lookups. A key-value store may be better suited for this. Portability. Little-endian architectures that run Linux are the only target.","title":"Overview"},{"location":"#noblit","text":"Vaporware warning: much of the content below is hypothetical. Noblit is an embeddable append-only database. The database records a history of immutable (entity, attribute, value) tuples. Tuples can be asserted and retracted. A retraction is recorded as a new fact; it is not a delete. Any historical state of the database can be reproduced, and the history is first-class and queryable.","title":"Noblit"},{"location":"#features","text":"Embeddable. Noblit comes as a library that you link into your application. Global installation or daemons are not necessary. Simple but flexible data model. Noblit stores (entity, attribute, value) tuples. A traditional relational model can be expressed like this, but less rigid structures such as graphs can be expressed too. Machine-friendly relational queries. Queries are data structures rather than strings. There is no need for string formatting or escaping, and the full power of the host language is available to safely generate and compose queries. Point in time queries. Any historical state of the database can be reproduced and queried efficiently. Reified schema . Noblit stores the schema as entities in the database itself. The schema can be evolved through normal assertions and retractions, and the full migration history is available. Reified transactions . Transactions in Noblit are entities that can have attributes like any other entity. Possibilities include the transaction timestamp and the user who initiated the transaction. Transactions can be inspected in queries. Profile-guided query optimization . While queries are declarative, statement order affects the query plan. An explicit query optimizer can optimize a given query by measuring how alternatives perform, instead of having to estimate from statistics.","title":"Features"},{"location":"#comparison","text":"Noblit is heavily inspired by Datomic , especially by its data model. In comparison to other databases such as SQLite and Postgres , Noblit positions itself as follows: Client-server Embedded Mutable Postgres SQLite Immutable Datomic Noblit Noblit combines the low operational overhead of an embedded database with the simplicity of an append-only database with value semantics. In comparison to Datomic, Noblit puts more focus on static type safety. This makes schemas more rigid. On the one hand this impedes iterating quickly, but on the other hand it helps to provide data consistency in the long term. Datomic is a distributed system running on the JVM , while Noblit is a small native library. Another attempt at an embedded database inspired by Datomic was Mentat . Both Noblit and Mentat happen to be written in Rust. Mentat is a layer on top of SQLite, whereas Noblit has its own storage backend. Mentat exists but is unmaintained, Noblit is vaporware.","title":"Comparison"},{"location":"#goals","text":"An embeddable library for queries and transactions. Enabling \u2014 but not requiring \u2014 a client-server database on top of the library, with a single transactor and scale-out reads. Storing datasets that do not fit in working memory. The result of a query and data to insert must fit though. Handling moderate write workloads and moderately sized data. Relational queries in a machine-first query format. Simplicity. First make it work, then make it fast. I am writing this down to remind myself to not micro-optimize. Complications should be justified by profiling measurements. Easy to build, few or zero dependencies.","title":"Goals"},{"location":"#non-goals","text":"Deleting data or updating data in place. Writing large values (say, larger than 64 KiB). A key-value store or document store might be better suited for this. High write throughput. If you produce large volumes of data quickly, probably not all of it remains valuable for a long time, and storage space might become a concern. A time series database or durable message queue might be a better alternative. Having the fastest lookups. A key-value store may be better suited for this. Portability. Little-endian architectures that run Linux are the only target.","title":"Non-goals"},{"location":"building/","text":"Building Noblit is a native library written in Rust. It will come with official client libraries for Haskell, Python, and Rust. The Haskell client is under construction; the Rust and Python clients do not yet exist. Build tools Nix can set up a build environment in which all required build tools are available. The repository contains a default.nix file that defines the environment. All build tools are pinned for reproducibility. The Nix environment is used for CI , so it is actively tested. There are three ways to use the build environment: Enter a shell in which all build tools are available with nix run -c $SHELL . Prefix all commands with nix run -c . Bring the binaries into your PATH with export PATH=$(nix-build --no-out-link)/bin:$PATH . Using Nix is convenient, but not a requirement. You can source your build tools elsewhere if you like. Noblit Noblit builds with Rust\u2019s build tool Cargo. Noblit is developed and tested against Rust 1.28.0, because this was the Rust version that the latest two Ubuntu LTSes as well as Debian Testing shipped at the time of its inception. Later versions of Rust may work. Noblit has no dependencies apart from the Rust standard library. To build: $ cargo build --release $ ls target/release/libnoblit* This will have produced three libraries: libnoblit.so , for dynamic linking against the C interface. libnoblit.a , for static linking against the C interface. libnoblit.rlib , for use in Rust programs. If you need a header file, a script can generate one from the C interface documentation : $ libnoblit/gen_header.py noblit.h Haskell client The Haskell client is located in client/haskell and builds with Stack. Currently the library supports Stackage LTS 13 (GHC 8.6). To build: $ cd client/haskell $ stack setup $ stack build TODO: How does it find the Rust lib? TODO: How to use in an application? Python client The Python client is located in client/python . It loads libnoblit.so using Python\u2019s ctypes module. If loading fails for the unqualified path, the library tries to load from target/debug and target/release to aid local development. Python code does not need to be compiled, but it can be typechecked by Mypy : $ mypy --strict client/python Rust client Rust programs can use the noblit crate in the noblit directory directly, although that exposes internals, and building queries is neithere convenient nor type safe. It would be nice to add a layer on top that hides most of the internals and exposes a more user-focused interface, but such a Rust client does not yet exist.","title":"Building"},{"location":"building/#building","text":"Noblit is a native library written in Rust. It will come with official client libraries for Haskell, Python, and Rust. The Haskell client is under construction; the Rust and Python clients do not yet exist.","title":"Building"},{"location":"building/#build-tools","text":"Nix can set up a build environment in which all required build tools are available. The repository contains a default.nix file that defines the environment. All build tools are pinned for reproducibility. The Nix environment is used for CI , so it is actively tested. There are three ways to use the build environment: Enter a shell in which all build tools are available with nix run -c $SHELL . Prefix all commands with nix run -c . Bring the binaries into your PATH with export PATH=$(nix-build --no-out-link)/bin:$PATH . Using Nix is convenient, but not a requirement. You can source your build tools elsewhere if you like.","title":"Build tools"},{"location":"building/#noblit","text":"Noblit builds with Rust\u2019s build tool Cargo. Noblit is developed and tested against Rust 1.28.0, because this was the Rust version that the latest two Ubuntu LTSes as well as Debian Testing shipped at the time of its inception. Later versions of Rust may work. Noblit has no dependencies apart from the Rust standard library. To build: $ cargo build --release $ ls target/release/libnoblit* This will have produced three libraries: libnoblit.so , for dynamic linking against the C interface. libnoblit.a , for static linking against the C interface. libnoblit.rlib , for use in Rust programs. If you need a header file, a script can generate one from the C interface documentation : $ libnoblit/gen_header.py noblit.h","title":"Noblit"},{"location":"building/#haskell-client","text":"The Haskell client is located in client/haskell and builds with Stack. Currently the library supports Stackage LTS 13 (GHC 8.6). To build: $ cd client/haskell $ stack setup $ stack build TODO: How does it find the Rust lib? TODO: How to use in an application?","title":"Haskell client"},{"location":"building/#python-client","text":"The Python client is located in client/python . It loads libnoblit.so using Python\u2019s ctypes module. If loading fails for the unqualified path, the library tries to load from target/debug and target/release to aid local development. Python code does not need to be compiled, but it can be typechecked by Mypy : $ mypy --strict client/python","title":"Python client"},{"location":"building/#rust-client","text":"Rust programs can use the noblit crate in the noblit directory directly, although that exposes internals, and building queries is neithere convenient nor type safe. It would be nice to add a layer on top that hides most of the internals and exposes a more user-focused interface, but such a Rust client does not yet exist.","title":"Rust client"},{"location":"data-model/","text":"Data model Noblit stores datoms . Datoms assert or retract attributes of entities. For example, entity 12 might have an attribute user.email with value rachael@tyrell.com . Conceptually, the database is an append-only log of (entity, attribute, value) tuples, together with the time at which they were asserted or retracted in the form of a transaction id. A view of the database at a given point in time is the set of all (entity, attribute, value) tuples that have been asserted and not retracted before or at that time. A datom is a tuple of the following five values: Entity : An integer that uniquely identifies an entity in the database. Attribute : Analogous to a column in a relational database. Value : The value for the attribute. Transaction : The transaction id of the transaction that added the datom. Operation : Either assert or retract . To get a view of the database at a given transaction t , we exclude all datoms with a transaction greater than t . Then we cancel assertions against subsequent retractions. What is left is the set of (entity, attribute, value) tuples that were true after transaction t .","title":"Data model"},{"location":"data-model/#data-model","text":"Noblit stores datoms . Datoms assert or retract attributes of entities. For example, entity 12 might have an attribute user.email with value rachael@tyrell.com . Conceptually, the database is an append-only log of (entity, attribute, value) tuples, together with the time at which they were asserted or retracted in the form of a transaction id. A view of the database at a given point in time is the set of all (entity, attribute, value) tuples that have been asserted and not retracted before or at that time. A datom is a tuple of the following five values: Entity : An integer that uniquely identifies an entity in the database. Attribute : Analogous to a column in a relational database. Value : The value for the attribute. Transaction : The transaction id of the transaction that added the datom. Operation : Either assert or retract . To get a view of the database at a given transaction t , we exclude all datoms with a transaction greater than t . Then we cancel assertions against subsequent retractions. What is left is the set of (entity, attribute, value) tuples that were true after transaction t .","title":"Data model"},{"location":"execution/","text":"Execution This page gives a high-level overview of how queries are executed. Rust\u2019s borrowing rules enforce a rigid structure on the ordering of operations. The components involved are: Indexes : The index trees and their backing storage. Because indexes are persistent data structures, it would be possible to have multiple readers and a single writer simultaneously. However, the current implementation requires exclusive access for writes, as this is easier to express in Rust. The heap : Large values live on the large value heap. Like indexes it is append-only, so it would admit multiple readers and a single writer simultaneously, but the current implementation requires exclusive access. The temporary heap : Query evaluation requires all large values to live on a heap. Large values that occur in queries are placed on a temporary heap. The temporary heap is a layer on top of the persistent large value heap: lookups for non-temporaries fall through to the underlying heap. Queries Create a temporary heap. Parse the query. Place values on the temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query. Release persistent indexes and heap (read-only). Drop the temporaries. Mutations Create a temporary heap. (TODO: Create two, one for reads, one for writes.) Parse the query. Place values that occur in the query part on the reads temporary heap. Place values that occur in assertions on the writes temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query part (if any, otherwise use unit). For every result, collect new datoms. Release persistent indexes and heap (read-only). Persist any temporaries referenced by new datoms. This requires write access to the persistent heap. Acquire the persistent heap, now containing necessary values (read-only). Persist new datoms. This requires write access to the persistent indexes. Release the persistent heap (read-only).","title":"Execution"},{"location":"execution/#execution","text":"This page gives a high-level overview of how queries are executed. Rust\u2019s borrowing rules enforce a rigid structure on the ordering of operations. The components involved are: Indexes : The index trees and their backing storage. Because indexes are persistent data structures, it would be possible to have multiple readers and a single writer simultaneously. However, the current implementation requires exclusive access for writes, as this is easier to express in Rust. The heap : Large values live on the large value heap. Like indexes it is append-only, so it would admit multiple readers and a single writer simultaneously, but the current implementation requires exclusive access. The temporary heap : Query evaluation requires all large values to live on a heap. Large values that occur in queries are placed on a temporary heap. The temporary heap is a layer on top of the persistent large value heap: lookups for non-temporaries fall through to the underlying heap.","title":"Execution"},{"location":"execution/#queries","text":"Create a temporary heap. Parse the query. Place values on the temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query. Release persistent indexes and heap (read-only). Drop the temporaries.","title":"Queries"},{"location":"execution/#mutations","text":"Create a temporary heap. (TODO: Create two, one for reads, one for writes.) Parse the query. Place values that occur in the query part on the reads temporary heap. Place values that occur in assertions on the writes temporary heap. Acquire persistent indexes and heap (read-only). Evaluate the query part (if any, otherwise use unit). For every result, collect new datoms. Release persistent indexes and heap (read-only). Persist any temporaries referenced by new datoms. This requires write access to the persistent heap. Acquire the persistent heap, now containing necessary values (read-only). Persist new datoms. This requires write access to the persistent indexes. Release the persistent heap (read-only).","title":"Mutations"},{"location":"files/","text":"Files Vaporware warning: much of the content below is hypothetical. Currently Noblit does not persist anything to disk at all. Noblit stores thee kinds of data on disk: The heap of large values (integers larger than 62 bits, and byte strings longer than 7 bytes). The indexes that store datoms in sorted order. The head with the most recent tree roots, and id counters. Accretion Because Noblit is an append-only database, transactions only add new datoms. Datoms are only stored in indexes (the indexes are covering ), but they may reference large values on the heap. Indexes are trees that consist of nodes. The indexes are persistent data structures, in the sense that data is immutable once written, but we can construct a new index that shares most of its nodes with a previous version. The index file is an append-only collection of nodes. The head points to the latest roots of the index trees, and it stores the counters for allocating entity ids. The head is the only part of Noblit that is updated in place, the other files are append-only. Committing a transaction is a three-stage process: Append any new large values to the heap file, and sync the heap. Add datoms to the indexes. This produces one or more new tree nodes per index. Append the new nodes to the index file, and sync it. Write the new head, and sync it. By making the head update atomic, the entire transaction becomes atomic. If the commit fails at some point before the new head is written, the old head is still valid, and it points to valid data. New data may have been appended, but that data is not yet referenced. The Indexes The indexes in Noblit store datoms in sorted order. There are three indexes: Eavt : sorted by entity, attribute, value, and transaction. Aevt : sorted by attribute, entity, value, and transaction. Avet : sorted by attribute, value, entity, and transaction. See also Datomic's index documentation , which formed the inspiration for Noblit. Note that unlike Datomic, Noblit does not have a Vaet (value, attribute, entity, transaction) index. This is because attributes in Noblit are strongly typed. A query such as \u201clist all attributes that this entity has\u201d is impossible to express in Noblit, because the values associated with the attribute may not have the same type. Each index stores all datoms in full. A datom is 32 bytes. Small values are stored inline in the datom, large values (integers larger than 62 bits and byte strings longer than 7 bytes) are stored on the value heap, with the datom containing the index into the value heap. Noblit stores indexes as hitchhiker trees , a variation on immutable B-trees which reduces write amplification. Noblit accumulates new datoms in memory first. At the end of a transaction it flushes those at once to the disk as new tree nodes, which may share child nodes with the previous tree. To prevent unreachable nodes from accumulating, trees need to be compacted occasionally though a copying garbage collection process. Unreachable nodes are not recycled to ensure immutability of written files: new data is only ever appended at the end. This simplifies caching. The Head For every index, the head stores the page id of the root of the tree for that index. Furthermore, the head stores the counters for id allocation. TODO: The head should store the size of the index file and heap file, so that it can truncate them after a failed transaction. The Heap The heap is a file that contains large values : integers larger than 62 bytes and byte strings longer than 7 bytes. Because Noblit is an append-only database that never removes data, storing a value on the heap is a simple bump-pointer allocation; the heap only grows. The heap stores two kinds of values: 64-bit integers which don't fit into 62 bytes. They are stored as-is, in 8 bytes, big endian. (TODO: It should be little endian.) Byte strings longer than 7 bytes. Note that e.g. strings are also stored as byte strings; it is the schema that specifies that those bytes should be interpreted as a UTF-8 encoded string. Byte strings are length-prefixed with a 64-bit length. The address of a byte string is the offset of its length prefix, so its data can be found at the offset 8 bytes higher. Values on the value heap are aligned to 8 bytes. The heap is not checksummed. (Nor the indexes for that matter.) If you do not trust your storage medium, use a file system or virtual block device that can detect and report integrity problems. The value heap might store duplicates. Because values are immutable once stored, deduplication is safe. If a Datom contains a value that already exists on the heap, it is safe to reference the existing value, rather than storing it again on the heap. Noblit may do this, but identifying duplicates is not free, hence Noblit may store the same value twice. TODO: Persist a hash table of values too, to identify duplicates? Not really, can read heap at startup, although that does not scale, persistent hash table may be needed.","title":"Files"},{"location":"files/#files","text":"Vaporware warning: much of the content below is hypothetical. Currently Noblit does not persist anything to disk at all. Noblit stores thee kinds of data on disk: The heap of large values (integers larger than 62 bits, and byte strings longer than 7 bytes). The indexes that store datoms in sorted order. The head with the most recent tree roots, and id counters.","title":"Files"},{"location":"files/#accretion","text":"Because Noblit is an append-only database, transactions only add new datoms. Datoms are only stored in indexes (the indexes are covering ), but they may reference large values on the heap. Indexes are trees that consist of nodes. The indexes are persistent data structures, in the sense that data is immutable once written, but we can construct a new index that shares most of its nodes with a previous version. The index file is an append-only collection of nodes. The head points to the latest roots of the index trees, and it stores the counters for allocating entity ids. The head is the only part of Noblit that is updated in place, the other files are append-only. Committing a transaction is a three-stage process: Append any new large values to the heap file, and sync the heap. Add datoms to the indexes. This produces one or more new tree nodes per index. Append the new nodes to the index file, and sync it. Write the new head, and sync it. By making the head update atomic, the entire transaction becomes atomic. If the commit fails at some point before the new head is written, the old head is still valid, and it points to valid data. New data may have been appended, but that data is not yet referenced.","title":"Accretion"},{"location":"files/#the-indexes","text":"The indexes in Noblit store datoms in sorted order. There are three indexes: Eavt : sorted by entity, attribute, value, and transaction. Aevt : sorted by attribute, entity, value, and transaction. Avet : sorted by attribute, value, entity, and transaction. See also Datomic's index documentation , which formed the inspiration for Noblit. Note that unlike Datomic, Noblit does not have a Vaet (value, attribute, entity, transaction) index. This is because attributes in Noblit are strongly typed. A query such as \u201clist all attributes that this entity has\u201d is impossible to express in Noblit, because the values associated with the attribute may not have the same type. Each index stores all datoms in full. A datom is 32 bytes. Small values are stored inline in the datom, large values (integers larger than 62 bits and byte strings longer than 7 bytes) are stored on the value heap, with the datom containing the index into the value heap. Noblit stores indexes as hitchhiker trees , a variation on immutable B-trees which reduces write amplification. Noblit accumulates new datoms in memory first. At the end of a transaction it flushes those at once to the disk as new tree nodes, which may share child nodes with the previous tree. To prevent unreachable nodes from accumulating, trees need to be compacted occasionally though a copying garbage collection process. Unreachable nodes are not recycled to ensure immutability of written files: new data is only ever appended at the end. This simplifies caching.","title":"The Indexes"},{"location":"files/#the-head","text":"For every index, the head stores the page id of the root of the tree for that index. Furthermore, the head stores the counters for id allocation. TODO: The head should store the size of the index file and heap file, so that it can truncate them after a failed transaction.","title":"The Head"},{"location":"files/#the-heap","text":"The heap is a file that contains large values : integers larger than 62 bytes and byte strings longer than 7 bytes. Because Noblit is an append-only database that never removes data, storing a value on the heap is a simple bump-pointer allocation; the heap only grows. The heap stores two kinds of values: 64-bit integers which don't fit into 62 bytes. They are stored as-is, in 8 bytes, big endian. (TODO: It should be little endian.) Byte strings longer than 7 bytes. Note that e.g. strings are also stored as byte strings; it is the schema that specifies that those bytes should be interpreted as a UTF-8 encoded string. Byte strings are length-prefixed with a 64-bit length. The address of a byte string is the offset of its length prefix, so its data can be found at the offset 8 bytes higher. Values on the value heap are aligned to 8 bytes. The heap is not checksummed. (Nor the indexes for that matter.) If you do not trust your storage medium, use a file system or virtual block device that can detect and report integrity problems. The value heap might store duplicates. Because values are immutable once stored, deduplication is safe. If a Datom contains a value that already exists on the heap, it is safe to reference the existing value, rather than storing it again on the heap. Noblit may do this, but identifying duplicates is not free, hence Noblit may store the same value twice. TODO: Persist a hash table of values too, to identify duplicates? Not really, can read heap at startup, although that does not scale, persistent hash table may be needed.","title":"The Heap"},{"location":"fuzz-tests/","text":"Fuzz tests Noblits internals are tested thoroughly through fuzz testing. Noblit uses structure-aware fuzzing to generate sequences of calls to internal API s, after which all invariants are checked. See the code in the fuzz directory. TODO: Expand these docs. TODO: Add more traditional fuzzer, for the parser of the binary query format. TODO: Add a structure-aware fuzzer for the higher-level API , in addition to those that focus on internals. Maybe just against the C interface. TODO: Add an abstraction for IO , and an implementation that that can inject errors based on the fuzz input.","title":"Fuzz tests"},{"location":"fuzz-tests/#fuzz-tests","text":"Noblits internals are tested thoroughly through fuzz testing. Noblit uses structure-aware fuzzing to generate sequences of calls to internal API s, after which all invariants are checked. See the code in the fuzz directory. TODO: Expand these docs. TODO: Add more traditional fuzzer, for the parser of the binary query format. TODO: Add a structure-aware fuzzer for the higher-level API , in addition to those that focus on internals. Maybe just against the C interface. TODO: Add an abstraction for IO , and an implementation that that can inject errors based on the fuzz input.","title":"Fuzz tests"},{"location":"golden-tests/","text":"Golden tests Noblit comes with a suite of golden test: queries with known-good reference results. The golden tests are stored in .t files in the golden directory. Each file contains a query and its expected outcome. Goldens can be verified with the test runner, golden/run.py . Example Below is a formatted example golden test that selects the entity id and name of every built-in type. It consists of the query, followed by the expected output: where t db.type.name name select t, name t name # 7 db.type.bool # 10 db.type.bytes # 8 db.type.ref # 11 db.type.string # 9 db.type.uint64 The above example is formatted for inclusion in the docs. In the .t files, expected output is included as a table drawn with box-drawing characters, as this is also what the execute binary prints. The encoding of the .t file is UTF-8 . Checking all goldens The test runner golden/run.py is desiged to be used with a TAP harness such as Prove . The runner prints TAP -compliant output to stdout. The runner can be used with Prove to verify all goldens: $ prove --exec golden/run.py golden This will run golden/run.py for every .t file in the golden directory, and print a summary of the results. Prove looks for .t files by default, which is also why Noblit uses that extension for goldens. You can get verbose output of the query and output by passing --verbose : $ prove --exec 'golden/run.py --verbose' --verbose golden Stages Checking a golden consists of several stages: The runner reads .t file and splits it into a query and expected outcome. The query is parsed by golden/parse.py , and serialized into a binary format. The execute binary parses the binary query and executes it. The runner pipes the binary query into the execute binary, and compares its output with the reference output. The parser and executor can also be used standalone. For example, to run the builtin types query: $ cat golden/builtin_types.t | golden/parse.py | target/debug/execute In this mode, the parser discards the reference output in the file.","title":"Golden tests"},{"location":"golden-tests/#golden-tests","text":"Noblit comes with a suite of golden test: queries with known-good reference results. The golden tests are stored in .t files in the golden directory. Each file contains a query and its expected outcome. Goldens can be verified with the test runner, golden/run.py .","title":"Golden tests"},{"location":"golden-tests/#example","text":"Below is a formatted example golden test that selects the entity id and name of every built-in type. It consists of the query, followed by the expected output: where t db.type.name name select t, name t name # 7 db.type.bool # 10 db.type.bytes # 8 db.type.ref # 11 db.type.string # 9 db.type.uint64 The above example is formatted for inclusion in the docs. In the .t files, expected output is included as a table drawn with box-drawing characters, as this is also what the execute binary prints. The encoding of the .t file is UTF-8 .","title":"Example"},{"location":"golden-tests/#checking-all-goldens","text":"The test runner golden/run.py is desiged to be used with a TAP harness such as Prove . The runner prints TAP -compliant output to stdout. The runner can be used with Prove to verify all goldens: $ prove --exec golden/run.py golden This will run golden/run.py for every .t file in the golden directory, and print a summary of the results. Prove looks for .t files by default, which is also why Noblit uses that extension for goldens. You can get verbose output of the query and output by passing --verbose : $ prove --exec 'golden/run.py --verbose' --verbose golden","title":"Checking all goldens"},{"location":"golden-tests/#stages","text":"Checking a golden consists of several stages: The runner reads .t file and splits it into a query and expected outcome. The query is parsed by golden/parse.py , and serialized into a binary format. The execute binary parses the binary query and executes it. The runner pipes the binary query into the execute binary, and compares its output with the reference output. The parser and executor can also be used standalone. For example, to run the builtin types query: $ cat golden/builtin_types.t | golden/parse.py | target/debug/execute In this mode, the parser discards the reference output in the file.","title":"Stages"},{"location":"htree/","text":"Trees Indexes in Noblit are hitchhiker trees . The trees have the same on-disk format as memory format, which allows them to be memory mapped. A hitchhiker tree is similar to an immutable B-tree, but updates usually only allocate a single new node, rather than log B (n) nodes. This gives us the advantages of an immutable data structure, without the write amplification. Tree nodes in Noblit are immutable. Index updates produce new nodes that succeed older nodes. Nodes can become unreachable, but they are never removed. Eventually, many nodes in a file may not be reachable. In that case the tree can be copied to a new file, omitting the unreachable blocks. This is a copying garbage collection. Index Trees Indexes in Noblit are sorted sets of datoms. Each datom is 32 bytes. (For large values, the datom contains a reference to a value on the heap.) The indexes store the datoms themselves: they are sorted sets, not key-values maps. In other words, indexes are covering indexes . Trees in Noblit store every datom exactly once. Datoms in leaves are not repeated in interior nodes, unlike a B+ tree, which would store all datoms in leaf nodes, and repeat some datoms as midpoints in interior nodes. In addition to the midpoint datoms, tree nodes store pending datoms: datoms that need to be flushed into the leaves, but which we avoid as long as possible. This modification is what turns a B-tree into a hitchhiker tree. Disk Format Tree nodes are 4096 bytes. Byte 4088 through 4095 contains the node header, which is built up of the following 8 bytes: Byte 0 contains the depth of the node (0 for a leaf, 1 for its parent, etc.). Byte 1 contains the number of datoms in the node internally, say k . k is at most 102. TODO: Leaf nodes could store 127 datoms and no child page ids, as opposed to 102 midpoints. The remaining 6 bytes are currently not used. TODO: I could store a checksum there. At byte 0, the datom array starts. It contains k datoms in increasing order. At byte 3264, the child array starts. It contains 64-bit page ids of the child nodes. The child array has k + 1 elements, such that all datoms in node children[i] order before datoms[i] . All datoms in node children[i + 1] order after datoms[i] . The special value 2 64 \u2013 1 indicates that there is no child node in this slot. If this is the case for slot i , then datom i is a pending datom rather than a midpoint, and that datom should be flushed to a child node if space runs out in the node.","title":"Trees"},{"location":"htree/#trees","text":"Indexes in Noblit are hitchhiker trees . The trees have the same on-disk format as memory format, which allows them to be memory mapped. A hitchhiker tree is similar to an immutable B-tree, but updates usually only allocate a single new node, rather than log B (n) nodes. This gives us the advantages of an immutable data structure, without the write amplification. Tree nodes in Noblit are immutable. Index updates produce new nodes that succeed older nodes. Nodes can become unreachable, but they are never removed. Eventually, many nodes in a file may not be reachable. In that case the tree can be copied to a new file, omitting the unreachable blocks. This is a copying garbage collection.","title":"Trees"},{"location":"htree/#index-trees","text":"Indexes in Noblit are sorted sets of datoms. Each datom is 32 bytes. (For large values, the datom contains a reference to a value on the heap.) The indexes store the datoms themselves: they are sorted sets, not key-values maps. In other words, indexes are covering indexes . Trees in Noblit store every datom exactly once. Datoms in leaves are not repeated in interior nodes, unlike a B+ tree, which would store all datoms in leaf nodes, and repeat some datoms as midpoints in interior nodes. In addition to the midpoint datoms, tree nodes store pending datoms: datoms that need to be flushed into the leaves, but which we avoid as long as possible. This modification is what turns a B-tree into a hitchhiker tree.","title":"Index Trees"},{"location":"htree/#disk-format","text":"Tree nodes are 4096 bytes. Byte 4088 through 4095 contains the node header, which is built up of the following 8 bytes: Byte 0 contains the depth of the node (0 for a leaf, 1 for its parent, etc.). Byte 1 contains the number of datoms in the node internally, say k . k is at most 102. TODO: Leaf nodes could store 127 datoms and no child page ids, as opposed to 102 midpoints. The remaining 6 bytes are currently not used. TODO: I could store a checksum there. At byte 0, the datom array starts. It contains k datoms in increasing order. At byte 3264, the child array starts. It contains 64-bit page ids of the child nodes. The child array has k + 1 elements, such that all datoms in node children[i] order before datoms[i] . All datoms in node children[i + 1] order after datoms[i] . The special value 2 64 \u2013 1 indicates that there is no child node in this slot. If this is the case for slot i , then datom i is a pending datom rather than a midpoint, and that datom should be flushed to a child node if space runs out in the node.","title":"Disk Format"},{"location":"query-optimization/","text":"Query optimization The order of statements in a query affect the query plan that Noblit chooses. As outlined in the Query planning chapter, every statement translates to a loop that scans over an index. The planner preserves statement order: the first statement becomes the outer loop, the last statement becomes the inner loop. Therefore, the statement order of the query can have a big effect on query performance. To help optimize the order of statements, Noblit features a query optimizer. The optimizer is not part of the query planner. It is a standalone function that takes a query and a database, and returns the optimized query. There are two ways to optimize a query plan: Optimizing statement order , also called macro-optimization , because statement order can make orders of magnitude difference in the evaluation time of a query. This is because statement order can make a difference in complexity, it can be the difference between a query plan that is quadratic in the size of an index, versus a query plan that is constant time. Statement order can be the difference between microseconds, and minutes. Choosing the indexes to use for a scan , also called micro-optimization , because so far empirical evidence suggests that the performance difference between the various indexes that can service a scan is small. This is because the indexes have the same complexity, it is only locality effects that make one index more suitable than another. The difference between the best and worst plan may be a factor two in extreme cases, but not an order of magnitude. Although the optimizer performs both macro and micro-optimization, currently the indexes used can not be controlled by the query. Perhaps it would be best for the optimizer to only consider plans that the planner generates. Optimizing statement order For a query with n statements, there are n! possible statement orders. Trying every permutation quickly becomes prohibitive, especially because many permutations lead to terrible query plans, so even trying each plan once can take a long time. Fortunately, with some reasonable assumptions, it is possible to find a good plan quickly. Assumption : Adding an extra statement at the end of a query, can not make it faster. This assumption is justified, because every statement translates to a scan over an index, so adding a statement results in strictly more work. With the above assumption, we can express statement order optimization as a tree search problem. The root of the tree is an empty query, and along the branches we include one more statement of the original query. Interior nodes of the tree are partial queries , while the leaves form all permutations of the statements. To find the optimal query plan is to pick the best leaf node, and the assumption we made implies that the query time of a child node is larger than the query time of the parent node. Given the tree of partial queries, we can explore the tree to find the leaf with the minimal query time. Because the query time of a child is greater than that of the parent, we can call the additional time the \u201ccost\u201d of an edge, and finding the leaf with the minimal query time means finding a path through the tree with minimal cost. Dijkstra\u2019s algorithm solves this neatly: Track an open set of candidate nodes. Remove the candidate with the minimal query time from the open set, and add all of its children. If the best candidate in the open set is a leaf node, then that is the optimal query plan. While this algorithm will find the optimal query plan, it is not obvious that it will find it quickly. The algorithm might explore breadth-first, and explore most of the tree before it reaches a leaf node. But in practice, this is not what happens. Early on, there tend to be a few partial queries that are so bad that they are worse than many leaf nodes. This cuts off entire branches of the tree, and the algorithm reaches a leaf node quickly. Optimizing scans Some scans can be serviced by multiple indexes. For example, for a partial index scan that finds the value of an attribute for a given entity, we could use either EAVT or AEVT . For a given statement order, the goal of scan optimization is to find the best index to use for each statement. Unlike statement order optimization, there is no property that allows us to find a good solution incrementally. Because of locality interactions between scans, we need to consider the query plan as a whole. The current approach is to just try all possibilities. It is exponential, but not nearly as bad as permutations of statements. A statement can be serviced by 1, 2, or 3 possible indexes, so the number of options tends to be in the dozens, which is quite doable. This will surely blow up if you enter a ridiculously long query. Perhaps deleting the micro-optimizer is the best remedy. Explore-exploit This section has not been written yet. The gist of it is: Benchmarking is hard. Take the minimum. Justify min * (1 - 1/sqrt(n)) ordering. Probablistic, so not shortest path, but no problem, we only want a good path.","title":"Query optimization"},{"location":"query-optimization/#query-optimization","text":"The order of statements in a query affect the query plan that Noblit chooses. As outlined in the Query planning chapter, every statement translates to a loop that scans over an index. The planner preserves statement order: the first statement becomes the outer loop, the last statement becomes the inner loop. Therefore, the statement order of the query can have a big effect on query performance. To help optimize the order of statements, Noblit features a query optimizer. The optimizer is not part of the query planner. It is a standalone function that takes a query and a database, and returns the optimized query. There are two ways to optimize a query plan: Optimizing statement order , also called macro-optimization , because statement order can make orders of magnitude difference in the evaluation time of a query. This is because statement order can make a difference in complexity, it can be the difference between a query plan that is quadratic in the size of an index, versus a query plan that is constant time. Statement order can be the difference between microseconds, and minutes. Choosing the indexes to use for a scan , also called micro-optimization , because so far empirical evidence suggests that the performance difference between the various indexes that can service a scan is small. This is because the indexes have the same complexity, it is only locality effects that make one index more suitable than another. The difference between the best and worst plan may be a factor two in extreme cases, but not an order of magnitude. Although the optimizer performs both macro and micro-optimization, currently the indexes used can not be controlled by the query. Perhaps it would be best for the optimizer to only consider plans that the planner generates.","title":"Query optimization"},{"location":"query-optimization/#optimizing-statement-order","text":"For a query with n statements, there are n! possible statement orders. Trying every permutation quickly becomes prohibitive, especially because many permutations lead to terrible query plans, so even trying each plan once can take a long time. Fortunately, with some reasonable assumptions, it is possible to find a good plan quickly. Assumption : Adding an extra statement at the end of a query, can not make it faster. This assumption is justified, because every statement translates to a scan over an index, so adding a statement results in strictly more work. With the above assumption, we can express statement order optimization as a tree search problem. The root of the tree is an empty query, and along the branches we include one more statement of the original query. Interior nodes of the tree are partial queries , while the leaves form all permutations of the statements. To find the optimal query plan is to pick the best leaf node, and the assumption we made implies that the query time of a child node is larger than the query time of the parent node. Given the tree of partial queries, we can explore the tree to find the leaf with the minimal query time. Because the query time of a child is greater than that of the parent, we can call the additional time the \u201ccost\u201d of an edge, and finding the leaf with the minimal query time means finding a path through the tree with minimal cost. Dijkstra\u2019s algorithm solves this neatly: Track an open set of candidate nodes. Remove the candidate with the minimal query time from the open set, and add all of its children. If the best candidate in the open set is a leaf node, then that is the optimal query plan. While this algorithm will find the optimal query plan, it is not obvious that it will find it quickly. The algorithm might explore breadth-first, and explore most of the tree before it reaches a leaf node. But in practice, this is not what happens. Early on, there tend to be a few partial queries that are so bad that they are worse than many leaf nodes. This cuts off entire branches of the tree, and the algorithm reaches a leaf node quickly.","title":"Optimizing statement order"},{"location":"query-optimization/#optimizing-scans","text":"Some scans can be serviced by multiple indexes. For example, for a partial index scan that finds the value of an attribute for a given entity, we could use either EAVT or AEVT . For a given statement order, the goal of scan optimization is to find the best index to use for each statement. Unlike statement order optimization, there is no property that allows us to find a good solution incrementally. Because of locality interactions between scans, we need to consider the query plan as a whole. The current approach is to just try all possibilities. It is exponential, but not nearly as bad as permutations of statements. A statement can be serviced by 1, 2, or 3 possible indexes, so the number of options tends to be in the dozens, which is quite doable. This will surely blow up if you enter a ridiculously long query. Perhaps deleting the micro-optimizer is the best remedy.","title":"Optimizing scans"},{"location":"query-optimization/#explore-exploit","text":"This section has not been written yet. The gist of it is: Benchmarking is hard. Take the minimum. Justify min * (1 - 1/sqrt(n)) ordering. Probablistic, so not shortest path, but no problem, we only want a good path.","title":"Explore-exploit"},{"location":"query-planning/","text":"Query planning Noblit features a straightforward and transparent query planner. On the one hand, this means that query performance can vary a lot depending on how the query is structured. On the other hand, this gives you a lot of control over the query plan, and it ensures predictible performance. Plans Noblit evaluates every query as a set of nested loops that scan over its indexes. Every where-statement in the query translates to one loop. The query plan determines the order of those loops, and which index to scan for each loop. There are three kinds of scan in Noblit: A full index scan. Such a scan can be used for statements where both the entity and the value are unknown. A full index scan provides two variables in one loop, but it may have to scan the entire index. For scans over attribute-leading indexes, the range is still constrained by attribute. A partial index scan. Such a scan can be used for statements where either the entity or the value is known, either because it was provided by an outer loop, or because it is a constant in the query. A partial index scan can be used to find a single datom (for example, to look up the value of an attribute for a known entity in the EAVT index), or to look up a range of datoms (for example, to get all entities where an attribute has a known value in the AVET index). An existence test. Such a scan can be used to test if a datom exists in the database when both the entity and value are known. If it does not exists, the loop associated with this scan will have zero iterations (for a given iteration of the outer loops). The variables in a statement determine which scan is used: If both the entity and value are variable, and neither of these variables was referenced in an earlier statement, a full index scan will be used. If the statement contains one variable that was not referenced in an earlier statement, a partial index scan will be used. If all variables in the statement were referenced in earlier statements, an existence test will be used. Consequently, the order of statements in the query completely specifies what kind of scans are used, and which variables are provided by every loop. There is, however, some freedom left to the planner. When a scan can be serviced by multiple indexes, the planner decides which one to use. For example, to look up the user.name attribute of a known entity, we could use either EAVT , or AEVT . The former facilitates row access like a traditional relational database, whereas the latter facilitates column access like in a column database. It depends on the full query which one is preferred. If we are only selecting user.name , then column access would be preferred. But if we are also selecting a few dozen other attributes of the user, then row access would provide better locality. The planner uses heuristics to select the index to use for each scan. TODO: The planner should take schema into account. For example, for a single-nonunique attribute, we expect a partial EAVT scan to be cheaper than a partial AVET scan. But for a many-unique attribute, the converse is true. Ordering TODO: Write about interaction between ordering constraints and the query plan. Advice Warning: The advice here is just a guess, and not based on measurements. Given the way that evaluation and the query planner work, it makes sense to choose a statement order that constrains the ranges scanned as much as possible, to minimize the number of loop iterations. Query optimization Noblit does provide a query optimizer. Unlike traditional relational databases, optimization is an explicit operation, it is not implicitly part of every query. With the burden of runtime query plan optimization out of the way, Noblit can focus on an off-line optimizer. This has several advantages over runtime query optimization: Because the runtime query planner is very simple, it is very fast. There is less of a trade-off between spending time on planning and spending time on evaluation. Off-line (in the sense that it runs when instructed to, not implicitly as part of every query) we can afford to spend more time optimizing. Now that we can afford to spend more time, the optimizer can measure , it does not have to guess. Instead of using statistics to try and estimate the cost of a given plan, Noblit can profile the query against your actual database. Now that the optimizer measures, rather than estimates, there is no need to keep statistics about the data to base the estimates on. This eliminates a lot of complexity. This optimization scheme is made possible by the control that a straightforward query planner provides. If the planner would perform more advanced runtime optimization, then there would be less opportunity to tell it exactly what kind of plan we want. Of course this optimization scheme has downsides too. In particular, off-line optimization is not always possible or desirable. For ad-hoc one-off queries, you have to keep performance in mind. When representative data is unavailable (for example, when you only have a test database, not production data), results of off-line optimization may not be representative of real-world performance. Rules of thumb can help to ensure reasonable plans, but are no substitute for profiling. There is no way to optimize generated or partially generated queries. What you can do though, is optimize a few generated queries, and use the lessons learned to tweak the generator. The internals of the query optimizer are outlined in the Query optimization chapter.","title":"Query planning"},{"location":"query-planning/#query-planning","text":"Noblit features a straightforward and transparent query planner. On the one hand, this means that query performance can vary a lot depending on how the query is structured. On the other hand, this gives you a lot of control over the query plan, and it ensures predictible performance.","title":"Query planning"},{"location":"query-planning/#plans","text":"Noblit evaluates every query as a set of nested loops that scan over its indexes. Every where-statement in the query translates to one loop. The query plan determines the order of those loops, and which index to scan for each loop. There are three kinds of scan in Noblit: A full index scan. Such a scan can be used for statements where both the entity and the value are unknown. A full index scan provides two variables in one loop, but it may have to scan the entire index. For scans over attribute-leading indexes, the range is still constrained by attribute. A partial index scan. Such a scan can be used for statements where either the entity or the value is known, either because it was provided by an outer loop, or because it is a constant in the query. A partial index scan can be used to find a single datom (for example, to look up the value of an attribute for a known entity in the EAVT index), or to look up a range of datoms (for example, to get all entities where an attribute has a known value in the AVET index). An existence test. Such a scan can be used to test if a datom exists in the database when both the entity and value are known. If it does not exists, the loop associated with this scan will have zero iterations (for a given iteration of the outer loops). The variables in a statement determine which scan is used: If both the entity and value are variable, and neither of these variables was referenced in an earlier statement, a full index scan will be used. If the statement contains one variable that was not referenced in an earlier statement, a partial index scan will be used. If all variables in the statement were referenced in earlier statements, an existence test will be used. Consequently, the order of statements in the query completely specifies what kind of scans are used, and which variables are provided by every loop. There is, however, some freedom left to the planner. When a scan can be serviced by multiple indexes, the planner decides which one to use. For example, to look up the user.name attribute of a known entity, we could use either EAVT , or AEVT . The former facilitates row access like a traditional relational database, whereas the latter facilitates column access like in a column database. It depends on the full query which one is preferred. If we are only selecting user.name , then column access would be preferred. But if we are also selecting a few dozen other attributes of the user, then row access would provide better locality. The planner uses heuristics to select the index to use for each scan. TODO: The planner should take schema into account. For example, for a single-nonunique attribute, we expect a partial EAVT scan to be cheaper than a partial AVET scan. But for a many-unique attribute, the converse is true.","title":"Plans"},{"location":"query-planning/#ordering","text":"TODO: Write about interaction between ordering constraints and the query plan.","title":"Ordering"},{"location":"query-planning/#advice","text":"Warning: The advice here is just a guess, and not based on measurements. Given the way that evaluation and the query planner work, it makes sense to choose a statement order that constrains the ranges scanned as much as possible, to minimize the number of loop iterations.","title":"Advice"},{"location":"query-planning/#query-optimization","text":"Noblit does provide a query optimizer. Unlike traditional relational databases, optimization is an explicit operation, it is not implicitly part of every query. With the burden of runtime query plan optimization out of the way, Noblit can focus on an off-line optimizer. This has several advantages over runtime query optimization: Because the runtime query planner is very simple, it is very fast. There is less of a trade-off between spending time on planning and spending time on evaluation. Off-line (in the sense that it runs when instructed to, not implicitly as part of every query) we can afford to spend more time optimizing. Now that we can afford to spend more time, the optimizer can measure , it does not have to guess. Instead of using statistics to try and estimate the cost of a given plan, Noblit can profile the query against your actual database. Now that the optimizer measures, rather than estimates, there is no need to keep statistics about the data to base the estimates on. This eliminates a lot of complexity. This optimization scheme is made possible by the control that a straightforward query planner provides. If the planner would perform more advanced runtime optimization, then there would be less opportunity to tell it exactly what kind of plan we want. Of course this optimization scheme has downsides too. In particular, off-line optimization is not always possible or desirable. For ad-hoc one-off queries, you have to keep performance in mind. When representative data is unavailable (for example, when you only have a test database, not production data), results of off-line optimization may not be representative of real-world performance. Rules of thumb can help to ensure reasonable plans, but are no substitute for profiling. There is no way to optimize generated or partially generated queries. What you can do though, is optimize a few generated queries, and use the lessons learned to tweak the generator. The internals of the query optimizer are outlined in the Query optimization chapter.","title":"Query optimization"},{"location":"query/","text":"Query Queries in Noblit are declarative, based on logic programming, inspired by Datalog. Queries consist of a number of statements (logical claims, not steps in a procedure) that relate variables. The result of a query are the possible assignments of values to the variables. Structure A query has three parts: Where , a collection of statements that are true of the answers. Select , the variables whose values will be returned. Order by , to control ordering. (Not implemented yet.) The where-part of a query consists of (entity, attribute, value) tuples. The entity and value can be variables or constants. By convention, below we use single-character variable names to refer to entities, and full names to refer to other types of values (strings and integers). Example As an example, select all tracks by Muse from a music database, ordered by release date and then by track number: where a artist.name \"Muse\" b album.artist a b album.title album_title b album.release.year year b album.release.month month b album.release.day day t track.album b t track.number number t track.title track_title select number, track_title, album_title order by year, month, day, number Select all tracks titled \"One\", their artist, and release year: where t track.title \"One\" t track.album b b album.release.year year b album.artist a a artist.name artist select artist, year Semantics To answer a query, identify every variable in the query with the set of values it can assume. The answer to the query is the cartesian product of these sets, filtered such that for every tuple in the product, a datom exists in the database for each of the where-statements. For example, consider the following database of (entity, attribute, value) pairs: 1 person.name \"Henk\" 2 person.name \"Klaas\" 3 person.name \"Piet\" 1 person.age 32 2 person.age 54 3 person.age 32 Suppose we want to find all pairs of people with the same age. We would query: where p person.age a q person.age a p person.name p_name q person.name q_name select p_name, q_name This query has five variables. Taking the Cartesian product, we get: p p_name q q_name a 1 Henk 1 Henk 32 2 Henk 1 Henk 32 ... ... 3 Piet 3 Piet 54 Filtering first by the first and last two statements ( p person.age a , p person.name p_name , and q person.name q_name ), we are left with only the tuples where a is the age of person p , and where the names and the person ids match: p p_name q q_name a 1 Henk 1 Henk 32 2 Klaas 1 Henk 52 ... ... 3 Piet 3 Piet 32 Filtering by the remaining statement q person.age a , we find all tuples that satisfy the query: p p_name q q_name a 1 Henk 1 Henk 32 3 Piet 1 Henk 32 2 Klaas 2 Klaas 52 3 Henk 3 Piet 32 3 Piet 3 Piet 32 Finally, keeping only the selected columns yields the answer: p_name q_name Henk Henk Piet Henk Klaas Klaas Henk Piet Piet Piet Aggregations Note: This is an idea, it has not been implemented yet. For aggregations such as sum , count , and min and max , we put the aggregate in the select part of the query. For example, in an order database, we could select the total amount billed in 2020: where i invoice.year 2020 i invoice.amount_eur amount_eur select sum(amount_eur) When mixing aggregates and non-aggregates, aggregates are taken over the group keyed on the non-aggregate variables. For example, we may compute the total amount billed per year: where i invoice.year year i invoice.amount_eur amount_eur select year, sum(amount_eur) The above query will return one row per distinct year, it will not return the same year twice. This is in contrast to the non-aggregate query, which would return a row for every invoice entity. Aggregates can always be streamed, the grouping behavior does never cause a collection to be materialized in memory. This is because a query is evaluated as a nested loop. Non-aggregated variables are on the outer loops, and aggregated variables on the inner loops. This means that we visit one group key at a time.","title":"Query"},{"location":"query/#query","text":"Queries in Noblit are declarative, based on logic programming, inspired by Datalog. Queries consist of a number of statements (logical claims, not steps in a procedure) that relate variables. The result of a query are the possible assignments of values to the variables.","title":"Query"},{"location":"query/#structure","text":"A query has three parts: Where , a collection of statements that are true of the answers. Select , the variables whose values will be returned. Order by , to control ordering. (Not implemented yet.) The where-part of a query consists of (entity, attribute, value) tuples. The entity and value can be variables or constants. By convention, below we use single-character variable names to refer to entities, and full names to refer to other types of values (strings and integers).","title":"Structure"},{"location":"query/#example","text":"As an example, select all tracks by Muse from a music database, ordered by release date and then by track number: where a artist.name \"Muse\" b album.artist a b album.title album_title b album.release.year year b album.release.month month b album.release.day day t track.album b t track.number number t track.title track_title select number, track_title, album_title order by year, month, day, number Select all tracks titled \"One\", their artist, and release year: where t track.title \"One\" t track.album b b album.release.year year b album.artist a a artist.name artist select artist, year","title":"Example"},{"location":"query/#semantics","text":"To answer a query, identify every variable in the query with the set of values it can assume. The answer to the query is the cartesian product of these sets, filtered such that for every tuple in the product, a datom exists in the database for each of the where-statements. For example, consider the following database of (entity, attribute, value) pairs: 1 person.name \"Henk\" 2 person.name \"Klaas\" 3 person.name \"Piet\" 1 person.age 32 2 person.age 54 3 person.age 32 Suppose we want to find all pairs of people with the same age. We would query: where p person.age a q person.age a p person.name p_name q person.name q_name select p_name, q_name This query has five variables. Taking the Cartesian product, we get: p p_name q q_name a 1 Henk 1 Henk 32 2 Henk 1 Henk 32 ... ... 3 Piet 3 Piet 54 Filtering first by the first and last two statements ( p person.age a , p person.name p_name , and q person.name q_name ), we are left with only the tuples where a is the age of person p , and where the names and the person ids match: p p_name q q_name a 1 Henk 1 Henk 32 2 Klaas 1 Henk 52 ... ... 3 Piet 3 Piet 32 Filtering by the remaining statement q person.age a , we find all tuples that satisfy the query: p p_name q q_name a 1 Henk 1 Henk 32 3 Piet 1 Henk 32 2 Klaas 2 Klaas 52 3 Henk 3 Piet 32 3 Piet 3 Piet 32 Finally, keeping only the selected columns yields the answer: p_name q_name Henk Henk Piet Henk Klaas Klaas Henk Piet Piet Piet","title":"Semantics"},{"location":"query/#aggregations","text":"Note: This is an idea, it has not been implemented yet. For aggregations such as sum , count , and min and max , we put the aggregate in the select part of the query. For example, in an order database, we could select the total amount billed in 2020: where i invoice.year 2020 i invoice.amount_eur amount_eur select sum(amount_eur) When mixing aggregates and non-aggregates, aggregates are taken over the group keyed on the non-aggregate variables. For example, we may compute the total amount billed per year: where i invoice.year year i invoice.amount_eur amount_eur select year, sum(amount_eur) The above query will return one row per distinct year, it will not return the same year twice. This is in contrast to the non-aggregate query, which would return a row for every invoice entity. Aggregates can always be streamed, the grouping behavior does never cause a collection to be materialized in memory. This is because a query is evaluated as a nested loop. Non-aggregated variables are on the outer loops, and aggregated variables on the inner loops. This means that we visit one group key at a time.","title":"Aggregations"},{"location":"resources/","text":"Resources Below are some resources that were helpful or inspiring for the development of Noblit. Some of them were not directly useful, but simply interesting. Deconstructing the Database \u2014 a talk about Datomic and its data model The Datomic Data Model \u2014 from the Datomic documentation LevelDB implementation notes \u2014 from the LevelDB documentation InfluxDB Storage Engine Internals \u2014 a talk about storage in a time series database What You Always Wanted to Know About Datalog \u2014 a paper on Datalog Unofficial Guide to Datomic Internals \u2014 a blog post on Datomic internals SQLite: A Database for the Edge of the Network \u2014 a talk about SQLite internals The CMU database group channel \u2014 recorded lectures from their database courses","title":"Resources"},{"location":"resources/#resources","text":"Below are some resources that were helpful or inspiring for the development of Noblit. Some of them were not directly useful, but simply interesting. Deconstructing the Database \u2014 a talk about Datomic and its data model The Datomic Data Model \u2014 from the Datomic documentation LevelDB implementation notes \u2014 from the LevelDB documentation InfluxDB Storage Engine Internals \u2014 a talk about storage in a time series database What You Always Wanted to Know About Datalog \u2014 a paper on Datalog Unofficial Guide to Datomic Internals \u2014 a blog post on Datomic internals SQLite: A Database for the Edge of the Network \u2014 a talk about SQLite internals The CMU database group channel \u2014 recorded lectures from their database courses","title":"Resources"},{"location":"examples/bug-tracker/","text":"Bug Tracker Example Note: This example is completely hypothetical, constructed to explore syntax and requirements. You cannot actually run these queries. Suppose we would want to build a database for a bug tracker, where users can file and comment on issues, and set and change certain properties. This is an excellent use case for an immutable database: we want to record status changes as new facts, but never lose historic data. Schema and Initial Provisioning We start by setting up attributes to define the schema, similar to how we would issue CREATE TABLE statements in a SQL database. Unlike in SQL, schema is reified in Noblit. This means that the schema is data, and it can be manipulated using the same query language as domain data. where $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" define attribute($a, $name, $type, $unique, $many): $a db.attribute.name $name $a db.attribute.type $type $a db.attribute.unique $unique $a db.attribute.many $many assert attribute($user_name, \"user.name\", $string_t, true, false) attribute($issue_title, \"issue.title\", $string_t, true, false) attribute($issue_description, \"issue.description\", $string_t, false, false) attribute($issue_priority, \"issue.priority\", $uint64_t, false, false) attribute($issue_author, \"issue.author\", $ref_t, false, false) attribute($issue_comment, \"issue.comment\", $ref_t, true, false) attribute($comment_content, \"comment.content\", $string_t, false, false) attribute($comment_author, \"comment.author\", $ref_t, false, false) where -- Lookp up the data types by name, so we can refer to these types when -- defining new attributes. $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" assert -- Note: these definitions are verbose. Noblit's query language does not -- offer abstractions such as functions to reduce verbosity. Rather, you -- can build abstractions on top of a client library. Because Noblit has -- a simple data model, and because Noblit accepts data structures rather -- than strings as queries, it is easy and safe to build such abstractions -- in your programming language, rather than into the query language. $author_name db.attribute.name \"author.name\" $author_name db.attribute.type $string_t $author_name db.attribute.unique true $author_name db.attribute.many false $issue_title db.attribute.name \"issue.title\" $issue_title db.attribute.type $string_t $issue_title db.attribute.unique true $issue_title db.attribute.many false $issue_description db.attribute.name \"issue.description\" $issue_description db.attribute.type $string_t $issue_description db.attribute.unique false $issue_description db.attribute.many false $issue_priority db.attribute.name \"issue.priority\" $issue_priority db.attribute.type $uint64_t $issue_priority db.attribute.unique false $issue_priority db.attribute.many false $issue_author db.attribute.name \"issue.author\" $issue_author db.attribute.type $ref_t $issue_author db.attribute.unique false $issue_author db.attribute.many false -- Experimental idea: attribute constraints (like foreign keys). -- The author ref must point to an entity that has the \"author.name\" -- attribute. $issue_author db.constraint.has_attribute $author_name $comment_content db.attribute.name \"comment.content\" $comment_content db.attribute.type $string_t $comment_content db.attribute.unique false $comment_content db.attribute.many false $comment_author db.attribute.name \"comment.author\" $comment_author db.attribute.type $ref_t $comment_author db.attribute.unique false $comment_author db.attribute.many false $comment_author db.constraint.has_attribute author_name -- TODO: Add created date_time_offset to issue and comment. -- Finally, an issue can have multiple comments associated with it. We -- could model this relation in two ways: by giving the comment a -- \"comment.issue\" attribute, or by giving the issue an \"issue.comment\" -- attribute. For the sake of demonstration we will go with the latter. -- Note that because Noblit maintains an (attribute, entity, value) index -- as well as an (attribute, value, entity) index, finding all comments -- associated with an issue is efficient in any case. $issue_comment db.attribute.name \"issue.comment\" $issue_comment db.attribute.type $ref_t $issue_comment db.attribute.unique false -- We set \"many\" to true: an issue can have many comments; the attribute -- may be present zero or more times. $issue_comment db.attribute.many true $issue_comment db.constraint.has_attribute $author_name select $issue_title, $issue_description, $issue_priority, $issue_author This will set up thee kinds of entities: authors, issues, and comments. These kinds are only encoded in the attribute names, there are no tables like in a traditional database. The attribute ids of issue.title , issue.description , issue.priority , and issue.author will be returned. Let's insert some initial data: assert $batty author.name \"Roy Batty\" $i issue.title \"Lifespan is too short\" $i issue.description \"Can the maker repair what he makes? I want more life.\" $i issue.priority 0 $i issue.author $batty select $i Now we have one issue. The query returns the entity id of the issue. Let's close the issue as wontfix. But wait a mintue ... we forgot to add a status attribute! Let's define one right now. where $string_t db.type.name \"string\" $ref_t db.type.name \"ref\" assert -- Define an enum for statuses, by defining a single \"status.name\" -- attribute that must have a unique value among all instances of this -- attribute. $status_name db.attribute.name \"status.name\" $status_name db.attribute.type $string_t $status_name db.attribute.unique true $status_name db.attribute.many false -- Then we define the issue statuses. We reference the new attribute by -- variable here. It could not be referenced by name, because at the time -- the query is constructed, it does not yet exist. $open_unconfirmed $status_name \"open/unconfirmed\" $open_confirmed $status_name \"open/confirmed\" $open_in_progress $status_name \"open/in-progress\" $closed_fixed $status_name \"closed/fixed\" $closed_obsolete $status_name \"closed/obsolete\" $closed_wontfix $status_name \"closed/wont-fix\" -- Finally, an issue can have a status. $issue_status db.attribute.name \"issue.status\" $issue_status db.attribute.type $ref_t $issue_status db.attribute.unique false $issue_status db.attribute.many false $issue_comment db.constraint.has_attribute $status_name Note that at this point, although she issue.status attribute exists, our first issue does not have the attribute. TODO: How to deal with implied attributes? (Sort of like non-nullable, except that there is no null in Noblit, only the presence or absence of an attribute.) Add a constraint that issue.title implies issue.status (and the others)? And then we'd be forced to add a status to all issues that don't have one in the same transaction that adds the attribute. In any case, let's set the issue to open/confirmed. We could reference the issue and status by entity id directly, but for demonstration purposes, we will look it up by title instead. We marked the issue.title and status.name attributes are unique, so this will affect at most one entity. where $issue issue.title \"Lifespan is too short\" $open status.name \"open/confirmed\" assert $issue issue.status $open Now we can close the issue. Note that because the issue.status attribute has its cardinality attribute.many set to false, we can't just assert a new status. We also have to retract the old status. The change is atomic, in one transaction. where $i issue.title \"Lifespan is too short\" $i issue.status $old_status $wontfix status.name \"closed/wont-fix\" retract $i issue.status $old_status assert $tyrell author.name \"Eldon Tyrell\" $c comment.content \"The light that burns twice as bright burns half as long.\" $c comment.author $tyrell $i issue.comment $c $i issue.status $wontfix A Selection of Selects Suppose we wanted to list all issues, ordered by priority: where $i issue.title $title $i issue.priority $p select $i, $p, $title order by $p asc This will return triples of issue entity id, priority, and title, ordered by priority (supposing we took lower integers to indicate more urgent issues). TODO: How to filter on only open issues? Would need an in or or clause. Suppose we wanted to list all issues created by Roy Batty: where $batty author.name \"Roy Batty\" $i issue.author $batty $i issue.title $title select $i, $title Now we get the entity ids of the issues created by Roy, as well as their titles. Suppose we want to find all issues where Rick Deckard left a comment: where $deckard author.name \"Rick Deckard\" $c comment.author $deckard $i issue.comment $c $i issue.title $title select $i, $title We again get all issues and titles. We will get a cartesian product of issues and comments by Deckard though: if Deckard commented twice, the issue will be in the result set twice. TODO: How to address that? select unique ? Or more fine-grained control? Or just order by the result and uniq it in the client library? Querying History After our issue was created, the status changed from open/unfonfirmed to closed/wont-fix . However, if we query just the status, we can only see the current status. The query where $i issue.title \"Lifespan is too short\" $i issue.status $status select $status returns closed/wont-fix . If we wanted to construct a timeline of the status changes that the issue underwent, we could include historic datoms like so: where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert select $t, $prev_status, $new_status order by $t asc In a regular where query, only datoms that have not been retracted will match. A where historic query works differently: it queries all datoms, and it matches both assertions and retractons alike. To filter these, a historical query takes 5-tuples rather than 3-tuples. In addition to (entity, attribute, value), the query takes a transaction id and operation (assert or retract). In this case, we select prev_status and new_status , such that the status prev_status was retracted on issue i in transaction t , and new_status was asserted in the same transaction. This gets us a list of status changes, ordered by transaction id. Note that the query to locate the issue by title is not in a where historic clause. If it were, we would get the status changes of all issues that had been called Lifespan too short a t some point, not only the history of the issue that currently has that title. Transactions in Noblit are reified. They are entities that can have attributes. Suppose we had added a transaction.date_time_offset attribute to every transaction, and also a transaction.initiator . Then we could also find out when the attribute changed, and who made that change. where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert where $t transaction.date_time_offset $time $t transaction.initiator $initiator $initiator author.name $initiator_name select $time, $initiator_name, $prev_status, $new_status order by $time asc, $t asc Now we get a nice chronological list of status changes, along with the name of the user who made the change. TODO: Rename author.name to user.name ? TODO: Add section define transaction attributes.","title":"Bug tracker"},{"location":"examples/bug-tracker/#bug-tracker-example","text":"Note: This example is completely hypothetical, constructed to explore syntax and requirements. You cannot actually run these queries. Suppose we would want to build a database for a bug tracker, where users can file and comment on issues, and set and change certain properties. This is an excellent use case for an immutable database: we want to record status changes as new facts, but never lose historic data.","title":"Bug Tracker Example"},{"location":"examples/bug-tracker/#schema-and-initial-provisioning","text":"We start by setting up attributes to define the schema, similar to how we would issue CREATE TABLE statements in a SQL database. Unlike in SQL, schema is reified in Noblit. This means that the schema is data, and it can be manipulated using the same query language as domain data. where $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" define attribute($a, $name, $type, $unique, $many): $a db.attribute.name $name $a db.attribute.type $type $a db.attribute.unique $unique $a db.attribute.many $many assert attribute($user_name, \"user.name\", $string_t, true, false) attribute($issue_title, \"issue.title\", $string_t, true, false) attribute($issue_description, \"issue.description\", $string_t, false, false) attribute($issue_priority, \"issue.priority\", $uint64_t, false, false) attribute($issue_author, \"issue.author\", $ref_t, false, false) attribute($issue_comment, \"issue.comment\", $ref_t, true, false) attribute($comment_content, \"comment.content\", $string_t, false, false) attribute($comment_author, \"comment.author\", $ref_t, false, false) where -- Lookp up the data types by name, so we can refer to these types when -- defining new attributes. $string_t db.type.name \"string\" $uint64_t db.type.name \"uint64\" $ref_t db.type.name \"ref\" assert -- Note: these definitions are verbose. Noblit's query language does not -- offer abstractions such as functions to reduce verbosity. Rather, you -- can build abstractions on top of a client library. Because Noblit has -- a simple data model, and because Noblit accepts data structures rather -- than strings as queries, it is easy and safe to build such abstractions -- in your programming language, rather than into the query language. $author_name db.attribute.name \"author.name\" $author_name db.attribute.type $string_t $author_name db.attribute.unique true $author_name db.attribute.many false $issue_title db.attribute.name \"issue.title\" $issue_title db.attribute.type $string_t $issue_title db.attribute.unique true $issue_title db.attribute.many false $issue_description db.attribute.name \"issue.description\" $issue_description db.attribute.type $string_t $issue_description db.attribute.unique false $issue_description db.attribute.many false $issue_priority db.attribute.name \"issue.priority\" $issue_priority db.attribute.type $uint64_t $issue_priority db.attribute.unique false $issue_priority db.attribute.many false $issue_author db.attribute.name \"issue.author\" $issue_author db.attribute.type $ref_t $issue_author db.attribute.unique false $issue_author db.attribute.many false -- Experimental idea: attribute constraints (like foreign keys). -- The author ref must point to an entity that has the \"author.name\" -- attribute. $issue_author db.constraint.has_attribute $author_name $comment_content db.attribute.name \"comment.content\" $comment_content db.attribute.type $string_t $comment_content db.attribute.unique false $comment_content db.attribute.many false $comment_author db.attribute.name \"comment.author\" $comment_author db.attribute.type $ref_t $comment_author db.attribute.unique false $comment_author db.attribute.many false $comment_author db.constraint.has_attribute author_name -- TODO: Add created date_time_offset to issue and comment. -- Finally, an issue can have multiple comments associated with it. We -- could model this relation in two ways: by giving the comment a -- \"comment.issue\" attribute, or by giving the issue an \"issue.comment\" -- attribute. For the sake of demonstration we will go with the latter. -- Note that because Noblit maintains an (attribute, entity, value) index -- as well as an (attribute, value, entity) index, finding all comments -- associated with an issue is efficient in any case. $issue_comment db.attribute.name \"issue.comment\" $issue_comment db.attribute.type $ref_t $issue_comment db.attribute.unique false -- We set \"many\" to true: an issue can have many comments; the attribute -- may be present zero or more times. $issue_comment db.attribute.many true $issue_comment db.constraint.has_attribute $author_name select $issue_title, $issue_description, $issue_priority, $issue_author This will set up thee kinds of entities: authors, issues, and comments. These kinds are only encoded in the attribute names, there are no tables like in a traditional database. The attribute ids of issue.title , issue.description , issue.priority , and issue.author will be returned. Let's insert some initial data: assert $batty author.name \"Roy Batty\" $i issue.title \"Lifespan is too short\" $i issue.description \"Can the maker repair what he makes? I want more life.\" $i issue.priority 0 $i issue.author $batty select $i Now we have one issue. The query returns the entity id of the issue. Let's close the issue as wontfix. But wait a mintue ... we forgot to add a status attribute! Let's define one right now. where $string_t db.type.name \"string\" $ref_t db.type.name \"ref\" assert -- Define an enum for statuses, by defining a single \"status.name\" -- attribute that must have a unique value among all instances of this -- attribute. $status_name db.attribute.name \"status.name\" $status_name db.attribute.type $string_t $status_name db.attribute.unique true $status_name db.attribute.many false -- Then we define the issue statuses. We reference the new attribute by -- variable here. It could not be referenced by name, because at the time -- the query is constructed, it does not yet exist. $open_unconfirmed $status_name \"open/unconfirmed\" $open_confirmed $status_name \"open/confirmed\" $open_in_progress $status_name \"open/in-progress\" $closed_fixed $status_name \"closed/fixed\" $closed_obsolete $status_name \"closed/obsolete\" $closed_wontfix $status_name \"closed/wont-fix\" -- Finally, an issue can have a status. $issue_status db.attribute.name \"issue.status\" $issue_status db.attribute.type $ref_t $issue_status db.attribute.unique false $issue_status db.attribute.many false $issue_comment db.constraint.has_attribute $status_name Note that at this point, although she issue.status attribute exists, our first issue does not have the attribute. TODO: How to deal with implied attributes? (Sort of like non-nullable, except that there is no null in Noblit, only the presence or absence of an attribute.) Add a constraint that issue.title implies issue.status (and the others)? And then we'd be forced to add a status to all issues that don't have one in the same transaction that adds the attribute. In any case, let's set the issue to open/confirmed. We could reference the issue and status by entity id directly, but for demonstration purposes, we will look it up by title instead. We marked the issue.title and status.name attributes are unique, so this will affect at most one entity. where $issue issue.title \"Lifespan is too short\" $open status.name \"open/confirmed\" assert $issue issue.status $open Now we can close the issue. Note that because the issue.status attribute has its cardinality attribute.many set to false, we can't just assert a new status. We also have to retract the old status. The change is atomic, in one transaction. where $i issue.title \"Lifespan is too short\" $i issue.status $old_status $wontfix status.name \"closed/wont-fix\" retract $i issue.status $old_status assert $tyrell author.name \"Eldon Tyrell\" $c comment.content \"The light that burns twice as bright burns half as long.\" $c comment.author $tyrell $i issue.comment $c $i issue.status $wontfix","title":"Schema and Initial Provisioning"},{"location":"examples/bug-tracker/#a-selection-of-selects","text":"Suppose we wanted to list all issues, ordered by priority: where $i issue.title $title $i issue.priority $p select $i, $p, $title order by $p asc This will return triples of issue entity id, priority, and title, ordered by priority (supposing we took lower integers to indicate more urgent issues). TODO: How to filter on only open issues? Would need an in or or clause. Suppose we wanted to list all issues created by Roy Batty: where $batty author.name \"Roy Batty\" $i issue.author $batty $i issue.title $title select $i, $title Now we get the entity ids of the issues created by Roy, as well as their titles. Suppose we want to find all issues where Rick Deckard left a comment: where $deckard author.name \"Rick Deckard\" $c comment.author $deckard $i issue.comment $c $i issue.title $title select $i, $title We again get all issues and titles. We will get a cartesian product of issues and comments by Deckard though: if Deckard commented twice, the issue will be in the result set twice. TODO: How to address that? select unique ? Or more fine-grained control? Or just order by the result and uniq it in the client library?","title":"A Selection of Selects"},{"location":"examples/bug-tracker/#querying-history","text":"After our issue was created, the status changed from open/unfonfirmed to closed/wont-fix . However, if we query just the status, we can only see the current status. The query where $i issue.title \"Lifespan is too short\" $i issue.status $status select $status returns closed/wont-fix . If we wanted to construct a timeline of the status changes that the issue underwent, we could include historic datoms like so: where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert select $t, $prev_status, $new_status order by $t asc In a regular where query, only datoms that have not been retracted will match. A where historic query works differently: it queries all datoms, and it matches both assertions and retractons alike. To filter these, a historical query takes 5-tuples rather than 3-tuples. In addition to (entity, attribute, value), the query takes a transaction id and operation (assert or retract). In this case, we select prev_status and new_status , such that the status prev_status was retracted on issue i in transaction t , and new_status was asserted in the same transaction. This gets us a list of status changes, ordered by transaction id. Note that the query to locate the issue by title is not in a where historic clause. If it were, we would get the status changes of all issues that had been called Lifespan too short a t some point, not only the history of the issue that currently has that title. Transactions in Noblit are reified. They are entities that can have attributes. Suppose we had added a transaction.date_time_offset attribute to every transaction, and also a transaction.initiator . Then we could also find out when the attribute changed, and who made that change. where $i issue.title \"Lifespan is too short\" where historic $i issue.status $prev_status $t retract $i issue.status $new_status $t assert where $t transaction.date_time_offset $time $t transaction.initiator $initiator $initiator author.name $initiator_name select $time, $initiator_name, $prev_status, $new_status order by $time asc, $t asc Now we get a nice chronological list of status changes, along with the name of the user who made the change. TODO: Rename author.name to user.name ? TODO: Add section define transaction attributes.","title":"Querying History"},{"location":"reference/c/","text":"C API reference Noblit exposes a C interface for use from non-Rust code. The C interface is an unsafe layer on top of the Rust crate that implements Noblit. The C interface is also written in Rust, but exposes unmangled symbols with C-compatible types. The C interface is intended to be used by client libraries and therefore minimal. There are no functions to construct or manipulate queries. Instead, Noblit accepts queries in a binary format, and expects client libraries to build and serialize queries. This ensures that only simple data types have to be passed across FFI boundaries, and it minimizes ownership transfer across FFI boundaries. This eliminates room for error and makes it easier to fuzz the remaining API . The Python and Haskell client libraries are built upon the C interface. noblit_t typedef struct Noblit noblit_t; An opaque database handle. Internally, this structure contains a noblit::Database , with type parameters fixed to one of three cases: In-memory . In-memory databases are always mutable. Immutable on-disk . The files can be opened in read-only mode. (Not yet implemented.) Mutable on-disk . The files need to be opened writeable. (Not yet implemented.) Furthermore, the noblit_t structure stores the most recent error that occurred, if any. In Rust, errors are tracked through the Result type, and these are returned to the C interface as a status code, with separate getters for the error details in case of an error. TODO: Implement this. noblit_db_read_packed noblit_t* noblit_db_read_packed(uint8_t const* fname, size_t fname_len); fname Character array with the file name to open. Should not include a null terminator. Borrowed immutably for the duration of the call. fname_len Length of the file name array in bytes. return value A pointer to a database struct. The pointer should be treated as an opaque pointer, Should be treated as an opaque noblit_db_free void noblit_db_free(noblit_t* db); Close the database, release associated resources, and deallocate the noblit_t struct.","title":"C API reference"},{"location":"reference/c/#c-api-reference","text":"Noblit exposes a C interface for use from non-Rust code. The C interface is an unsafe layer on top of the Rust crate that implements Noblit. The C interface is also written in Rust, but exposes unmangled symbols with C-compatible types. The C interface is intended to be used by client libraries and therefore minimal. There are no functions to construct or manipulate queries. Instead, Noblit accepts queries in a binary format, and expects client libraries to build and serialize queries. This ensures that only simple data types have to be passed across FFI boundaries, and it minimizes ownership transfer across FFI boundaries. This eliminates room for error and makes it easier to fuzz the remaining API . The Python and Haskell client libraries are built upon the C interface.","title":"C API reference"},{"location":"reference/c/#noblit_t","text":"typedef struct Noblit noblit_t; An opaque database handle. Internally, this structure contains a noblit::Database , with type parameters fixed to one of three cases: In-memory . In-memory databases are always mutable. Immutable on-disk . The files can be opened in read-only mode. (Not yet implemented.) Mutable on-disk . The files need to be opened writeable. (Not yet implemented.) Furthermore, the noblit_t structure stores the most recent error that occurred, if any. In Rust, errors are tracked through the Result type, and these are returned to the C interface as a status code, with separate getters for the error details in case of an error. TODO: Implement this.","title":"noblit_t"},{"location":"reference/c/#noblit_db_read_packed","text":"noblit_t* noblit_db_read_packed(uint8_t const* fname, size_t fname_len); fname Character array with the file name to open. Should not include a null terminator. Borrowed immutably for the duration of the call. fname_len Length of the file name array in bytes. return value A pointer to a database struct. The pointer should be treated as an opaque pointer, Should be treated as an opaque","title":"noblit_db_read_packed"},{"location":"reference/c/#noblit_db_free","text":"void noblit_db_free(noblit_t* db); Close the database, release associated resources, and deallocate the noblit_t struct.","title":"noblit_db_free"},{"location":"reference/rust/","text":"Rust API reference The Rust API reference can be generated from the source code by running cargo doc in the repository root. Currently it is not hosted anywhere.","title":"Rust API reference"},{"location":"reference/rust/#rust-api-reference","text":"The Rust API reference can be generated from the source code by running cargo doc in the repository root. Currently it is not hosted anywhere.","title":"Rust API reference"},{"location":"theme/README.theme/","text":"Kilsbergen A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more. Demos Noblit documentation Pris documentation Tako documentation Features Responsive design Zero javascript License Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"Kilsbergen"},{"location":"theme/README.theme/#kilsbergen","text":"A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more.","title":"Kilsbergen"},{"location":"theme/README.theme/#demos","text":"Noblit documentation Pris documentation Tako documentation","title":"Demos"},{"location":"theme/README.theme/#features","text":"Responsive design Zero javascript","title":"Features"},{"location":"theme/README.theme/#license","text":"Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"License"},{"location":"README.theme/","text":"Kilsbergen A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more. Demos Noblit documentation Pris documentation Tako documentation Features Responsive design Zero javascript License Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"Kilsbergen"},{"location":"README.theme/#kilsbergen","text":"A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more.","title":"Kilsbergen"},{"location":"README.theme/#demos","text":"Noblit documentation Pris documentation Tako documentation","title":"Demos"},{"location":"README.theme/#features","text":"Responsive design Zero javascript","title":"Features"},{"location":"README.theme/#license","text":"Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"License"}]}